{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Umbomatumbo/Bachelor-Thesis/blob/main/RAGnlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLi09z_3qW8O"
      },
      "source": [
        "# **Project part II: retrieval-augmented generation**\n",
        "\n",
        "## _Computer Engineering - Natural language Pocessing_\n",
        "\n",
        "### Students\n",
        "- **David Petrovic** (ID: 2092073)\n",
        "- **Umberto Salviati** (ID: 2091685)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcL8585DsZML"
      },
      "source": [
        "### Preparatiom\n",
        "\n",
        "Dowloading the requirements for this notebook\n",
        "\n",
        "**Note:** This notebook is tested on **Colab**. All dependencies already present in Colab are not reinstalled.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "vQa6rjDLqPCI"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"langchain\" \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.41.3\" \"trl==0.4.7\" \"safetensors>=0.3.1\" \"langchain-community\" \"langchain-core\" \"pymupdf\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "markdown_document = \"# Foo\\n\\n    ## Bar\\n\\nHi this is Jim\\n\\nHi this is Joe\\n\\n ### Boo \\n\\n Hi this is Lance \\n\\n ## Baz\\n\\n Hi this is Molly\"\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=headers_to_split_on, strip_headers=True\n",
        ")\n",
        "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
        "md_header_splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qra-zUxvxvMd",
        "outputId": "650f9165-d41f-400d-b629-7ab8c148b52e"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Hi this is Jim  \\nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),\n",
              " Document(page_content='Hi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),\n",
              " Document(page_content='Hi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLrL342RtWxf"
      },
      "source": [
        "### Task 1: Data Preparation and  Cleaning\n",
        "\n",
        "\n",
        "In this task we'll be collecting, and then parsing, our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_and_tables_from_pdf_to_markdown(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract text and tables from a PDF and convert them into Markdown format,\n",
        "    maintaining the position of tables relative to the text. Also, detect titles,\n",
        "    section headers, and subsections to format them appropriately in Markdown.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    # Skip the first 11 pages\n",
        "    for i in range(11):\n",
        "        doc.delete_page(0)\n",
        "    # Delete the last 38 pages\n",
        "    for i in range(38):\n",
        "        doc.delete_page(-1)\n",
        "\n",
        "    markdown_text = \"\"\n",
        "    row_count = 0  # counts table rows\n",
        "\n",
        "    for page_num, page in enumerate(doc):\n",
        "        #if page_num > 1 : continue JUST FOR DEBUG\n",
        "        # Extract the full text with positioning\n",
        "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "        blocks.sort(key=lambda b: b['bbox'][1])  # sort blocks by vertical position\n",
        "\n",
        "        # Placeholder to collect table blocks\n",
        "        table_blocks = []\n",
        "        for table in page.find_tables():\n",
        "            table_blocks.append({\n",
        "                'bbox': table.bbox,\n",
        "                'content': table\n",
        "            })\n",
        "        firstLine = True\n",
        "        block_index = 0\n",
        "        table_index = 0\n",
        "        while block_index < len(blocks) or table_index < len(table_blocks):\n",
        "            if table_index < len(table_blocks) and (block_index >= len(blocks) or table_blocks[table_index]['bbox'][1] < blocks[block_index]['bbox'][1]):\n",
        "                # Process table block\n",
        "                table = table_blocks[table_index]['content']\n",
        "\n",
        "                # Extract the table header if present\n",
        "                if table.header:\n",
        "                    header = (\n",
        "                        \"| \" + \" | \".join(\n",
        "                            [name if name is not None else \"\" for name in table.header.names]\n",
        "                        ) + \" |\\n\"\n",
        "                        + \"| \" + \" | \".join([\"---\" for _ in table.header.names]) + \" |\\n\"\n",
        "                    )\n",
        "                    markdown_text += header\n",
        "                    row_count += 1\n",
        "\n",
        "                # Extract the table body\n",
        "                for row in table.extract():\n",
        "                    row_text = (\n",
        "                        \"| \" + \" | \".join([cell if cell is not None else \"\" for cell in row]) + \" |\\n\"\n",
        "                    )\n",
        "                    markdown_text += row_text\n",
        "                    row_count += 1\n",
        "\n",
        "                table_index += 1\n",
        "            else:\n",
        "                # Process text block\n",
        "                title = False\n",
        "                block = blocks[block_index]#if blok index = 0 then blok index++\n",
        "                if 'lines' in block:\n",
        "                    block_text = \"\"\n",
        "                    for line in block['lines']:\n",
        "                        for span in line['spans']:\n",
        "                            text = span['text']\n",
        "                            if text == '\\n' : continue\n",
        "                            font_size = span['size']\n",
        "                            # Detect titles and headers by font size (adjust the threshold as needed)\n",
        "                            if font_size > 18:  # Assuming title font size is greater than 19\n",
        "                                #if text is equalt to a number then it is a page number\n",
        "                                if text.isdigit() : continue\n",
        "                                if not title :\n",
        "                                   block_text += f\"# {text}\\n\\n\"\n",
        "                                   title = True\n",
        "                                else :\n",
        "                                   block_text = block_text[:-2]\n",
        "                                   block_text += f\" {text}\\n\\n\"\n",
        "                            elif font_size > 12:  # Assuming header font size is greater than 13\n",
        "                                block_text += f\"## {text}\\n\\n\"\n",
        "                                title = False\n",
        "                            elif font_size > 10:  # Assuming subsection font size is greater than 11\n",
        "                                if text == \"CHAPTER\" : continue\n",
        "                                block_text += f\"### {text}\\n\\n\"\n",
        "                                title = False\n",
        "                            else:\n",
        "                                block_text += text.replace(\"\\n\", \"  \\n\") + \" \"\n",
        "                                title = False\n",
        "                    if firstLine :\n",
        "                        print(\"First Line :\")\n",
        "                        print(block_text)\n",
        "                        block_text = \"\"\n",
        "                        firstLine = False\n",
        "                    markdown_text += block_text + \" \\n\\n\"\n",
        "                block_index += 1\n",
        "        print(f\"Processed page {page_num + 1} of {doc.page_count}.\")\n",
        "\n",
        "    doc.close()\n",
        "    print(f\"Loaded {row_count} table rows from file '{pdf_path}'.\\n\")\n",
        "    return markdown_text\n"
      ],
      "metadata": {
        "id": "0mXv2iGWC5zv"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "ruax_eiPgDBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a0a78b5e-30ec-412e-ed91-8423d6b005b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Line :\n",
            "4 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 1 of 528.\n",
            "First Line :\n",
            "2.1 • R EGULAR  E XPRESSIONS 5 \n",
            "Processed page 2 of 528.\n",
            "First Line :\n",
            "6 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 3 of 528.\n",
            "First Line :\n",
            "2.1 • R EGULAR  E XPRESSIONS 7 \n",
            "Processed page 4 of 528.\n",
            "First Line :\n",
            "8 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 5 of 528.\n",
            "First Line :\n",
            "2.1 • R EGULAR  E XPRESSIONS 9 \n",
            "Processed page 6 of 528.\n",
            "First Line :\n",
            "10 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 7 of 528.\n",
            "First Line :\n",
            "2.1 • R EGULAR  E XPRESSIONS 11 \n",
            "Processed page 8 of 528.\n",
            "First Line :\n",
            "12 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 9 of 528.\n",
            "First Line :\n",
            "2.2 • W ORDS 13 \n",
            "Processed page 10 of 528.\n",
            "First Line :\n",
            "14 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 11 of 528.\n",
            "First Line :\n",
            "2.3 • C ORPORA 15 \n",
            "Processed page 12 of 528.\n",
            "First Line :\n",
            "16 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 13 of 528.\n",
            "First Line :\n",
            "2.4 • S IMPLE  U NIX  T OOLS FOR  W ORD  T OKENIZATION 17 \n",
            "Processed page 14 of 528.\n",
            "First Line :\n",
            "18 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 15 of 528.\n",
            "First Line :\n",
            "2.5 • W ORD  T OKENIZATION 19 \n",
            "Processed page 16 of 528.\n",
            "First Line :\n",
            "20 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 17 of 528.\n",
            "First Line :\n",
            "2.5 • W ORD  T OKENIZATION 21 \n",
            "Processed page 18 of 528.\n",
            "First Line :\n",
            "22 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 19 of 528.\n",
            "First Line :\n",
            "2.6 • W ORD  N ORMALIZATION , L EMMATIZATION AND  S TEMMING 23 \n",
            "Processed page 20 of 528.\n",
            "First Line :\n",
            "24 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 21 of 528.\n",
            "First Line :\n",
            "2.7 • S ENTENCE  S EGMENTATION 25 \n",
            "Processed page 22 of 528.\n",
            "First Line :\n",
            "26 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 23 of 528.\n",
            "First Line :\n",
            "2.8 • M INIMUM  E DIT  D ISTANCE 27 \n",
            "Processed page 24 of 528.\n",
            "First Line :\n",
            "28 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 25 of 528.\n",
            "First Line :\n",
            "2.9 • S UMMARY 29 \n",
            "Processed page 26 of 528.\n",
            "First Line :\n",
            "30 C HAPTER  2 • R EGULAR  E XPRESSIONS , T EXT  N ORMALIZATION , E DIT  D ISTANCE \n",
            "Processed page 27 of 528.\n",
            "First Line :\n",
            "E XERCISES 31 \n",
            "Processed page 28 of 528.\n",
            "First Line :\n",
            "32 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 29 of 528.\n",
            "First Line :\n",
            "3.1 • N-G RAMS 33 \n",
            "Processed page 30 of 528.\n",
            "First Line :\n",
            "34 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 31 of 528.\n",
            "First Line :\n",
            "3.1 • N-G RAMS 35 \n",
            "Processed page 32 of 528.\n",
            "First Line :\n",
            "36 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 33 of 528.\n",
            "First Line :\n",
            "3.1 • N-G RAMS 37 \n",
            "Processed page 34 of 528.\n",
            "First Line :\n",
            "38 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 35 of 528.\n",
            "First Line :\n",
            "3.3 • E VALUATING  L ANGUAGE  M ODELS : P ERPLEXITY 39 \n",
            "Processed page 36 of 528.\n",
            "First Line :\n",
            "40 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 37 of 528.\n",
            "First Line :\n",
            "3.3 • E VALUATING  L ANGUAGE  M ODELS : P ERPLEXITY 41 \n",
            "Processed page 38 of 528.\n",
            "First Line :\n",
            "42 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 39 of 528.\n",
            "First Line :\n",
            "3.5 • G ENERALIZATION AND  Z EROS 43 \n",
            "Processed page 40 of 528.\n",
            "First Line :\n",
            "44 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 41 of 528.\n",
            "First Line :\n",
            "3.6 • S MOOTHING 45 \n",
            "Processed page 42 of 528.\n",
            "First Line :\n",
            "46 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 43 of 528.\n",
            "First Line :\n",
            "3.6 • S MOOTHING 47 \n",
            "Processed page 44 of 528.\n",
            "First Line :\n",
            "48 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 45 of 528.\n",
            "First Line :\n",
            "3.6 • S MOOTHING 49 \n",
            "Processed page 46 of 528.\n",
            "First Line :\n",
            "50 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 47 of 528.\n",
            "First Line :\n",
            "3.8 • A DVANCED : K NESER -N EY  S MOOTHING 51 \n",
            "Processed page 48 of 528.\n",
            "First Line :\n",
            "52 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 49 of 528.\n",
            "First Line :\n",
            "3.8 • A DVANCED : K NESER -N EY  S MOOTHING 53 \n",
            "Processed page 50 of 528.\n",
            "First Line :\n",
            "54 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 51 of 528.\n",
            "First Line :\n",
            "3.9 • A DVANCED : P ERPLEXITY ’ S  R ELATION TO  E NTROPY 55 \n",
            "Processed page 52 of 528.\n",
            "First Line :\n",
            "56 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 53 of 528.\n",
            "First Line :\n",
            "3.10 • S UMMARY 57 \n",
            "Processed page 54 of 528.\n",
            "First Line :\n",
            "58 C HAPTER  3 • N- GRAM  L ANGUAGE  M ODELS \n",
            "Processed page 55 of 528.\n",
            "First Line :\n",
            "E XERCISES 59 \n",
            "Processed page 56 of 528.\n",
            "First Line :\n",
            "60 C HAPTER  4 • N AIVE  B AYES , T EXT  C LASSIFICATION ,  AND  S ENTIMENT \n",
            "Processed page 57 of 528.\n",
            "First Line :\n",
            "4.1 • N AIVE  B AYES  C LASSIFIERS 61 \n",
            "Processed page 58 of 528.\n",
            "First Line :\n",
            "62 C HAPTER  4 • N AIVE  B AYES , T EXT  C LASSIFICATION ,  AND  S ENTIMENT \n",
            "Processed page 59 of 528.\n",
            "First Line :\n",
            "4.1 • N AIVE  B AYES  C LASSIFIERS 63 \n",
            "Processed page 60 of 528.\n",
            "First Line :\n",
            "64 C HAPTER  4 • N AIVE  B AYES , T EXT  C LASSIFICATION ,  AND  S ENTIMENT \n",
            "Processed page 61 of 528.\n",
            "First Line :\n",
            "4.2 • T RAINING THE  N AIVE  B AYES  C LASSIFIER 65 \n",
            "Processed page 62 of 528.\n",
            "First Line :\n",
            "66 C HAPTER  4 • N AIVE  B AYES , T EXT  C LASSIFICATION ,  AND  S ENTIMENT \n",
            "Processed page 63 of 528.\n",
            "First Line :\n",
            "4.4 • O PTIMIZING FOR  S ENTIMENT  A NALYSIS 67 \n",
            "Processed page 64 of 528.\n",
            "First Line :\n",
            "68 C HAPTER  4 • N AIVE  B AYES , T EXT  C LASSIFICATION ,  AND  S ENTIMENT \n",
            "Processed page 65 of 528.\n",
            "First Line :\n",
            "4.6 • N AIVE  B AYES AS A  L ANGUAGE  M ODEL 69 \n",
            "Processed page 66 of 528.\n",
            "First Line :\n",
            "70 C HAPTER  4 • N AIVE  B AYES , T EXT  C LASSIFICATION ,  AND  S ENTIMENT \n",
            "Processed page 67 of 528.\n",
            "First Line :\n",
            "4.7 • E VALUATION : P RECISION , R ECALL , F- MEASURE 71 \n",
            "Processed page 68 of 528.\n",
            "First Line :\n",
            "72 C HAPTER  4 • N AIVE  B AYES , T EXT  C LASSIFICATION ,  AND  S ENTIMENT \n",
            "Processed page 69 of 528.\n",
            "First Line :\n",
            "4.9 • S TATISTICAL  S IGNIFICANCE  T ESTING 73 \n",
            "Processed page 70 of 528.\n",
            "First Line :\n",
            "74 C HAPTER  4 • N AIVE  B AYES , T EXT  C LASSIFICATION ,  AND  S ENTIMENT \n",
            "Processed page 71 of 528.\n",
            "First Line :\n",
            "4.9 • S TATISTICAL  S IGNIFICANCE  T ESTING 75 \n",
            "Processed page 72 of 528.\n",
            "First Line :\n",
            "76 C HAPTER  4 • N AIVE  B AYES , T EXT  C LASSIFICATION ,  AND  S ENTIMENT \n",
            "Processed page 73 of 528.\n",
            "First Line :\n",
            "4.10 • A VOIDING  H ARMS IN  C LASSIFICATION 77 \n",
            "Processed page 74 of 528.\n",
            "First Line :\n",
            "78 C HAPTER  4 • N AIVE  B AYES , T EXT  C LASSIFICATION ,  AND  S ENTIMENT \n",
            "Processed page 75 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 79 \n",
            "Processed page 76 of 528.\n",
            "First Line :\n",
            "80 C HAPTER  4 • N AIVE  B AYES , T EXT  C LASSIFICATION ,  AND  S ENTIMENT \n",
            "Processed page 77 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 78 of 528.\n",
            "First Line :\n",
            "82 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 79 of 528.\n",
            "First Line :\n",
            "5.1 • T HE SIGMOID FUNCTION 83 \n",
            "Processed page 80 of 528.\n",
            "First Line :\n",
            "84 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 81 of 528.\n",
            "First Line :\n",
            "5.2 • C LASSIFICATION WITH  L OGISTIC  R EGRESSION 85 \n",
            "Processed page 82 of 528.\n",
            "First Line :\n",
            "86 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 83 of 528.\n",
            "First Line :\n",
            "5.2 • C LASSIFICATION WITH  L OGISTIC  R EGRESSION 87 \n",
            "Processed page 84 of 528.\n",
            "First Line :\n",
            "88 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 85 of 528.\n",
            "First Line :\n",
            "5.3 • M ULTINOMIAL LOGISTIC REGRESSION 89 \n",
            "Processed page 86 of 528.\n",
            "First Line :\n",
            "90 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 87 of 528.\n",
            "First Line :\n",
            "5.4 • L EARNING IN  L OGISTIC  R EGRESSION 91 \n",
            "Processed page 88 of 528.\n",
            "First Line :\n",
            "92 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 89 of 528.\n",
            "First Line :\n",
            "5.6 • G RADIENT  D ESCENT 93 \n",
            "Processed page 90 of 528.\n",
            "First Line :\n",
            "94 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 91 of 528.\n",
            "First Line :\n",
            "5.6 • G RADIENT  D ESCENT 95 \n",
            "Processed page 92 of 528.\n",
            "First Line :\n",
            "96 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 93 of 528.\n",
            "First Line :\n",
            "5.6 • G RADIENT  D ESCENT 97 \n",
            "Processed page 94 of 528.\n",
            "First Line :\n",
            "98 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 95 of 528.\n",
            "First Line :\n",
            "5.7 • R EGULARIZATION 99 \n",
            "Processed page 96 of 528.\n",
            "First Line :\n",
            "100 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 97 of 528.\n",
            "First Line :\n",
            "5.8 • L EARNING IN  M ULTINOMIAL  L OGISTIC  R EGRESSION 101 \n",
            "Processed page 98 of 528.\n",
            "First Line :\n",
            "102 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 99 of 528.\n",
            "First Line :\n",
            "5.11 • S UMMARY 103 \n",
            "Processed page 100 of 528.\n",
            "First Line :\n",
            "104 C HAPTER  5 • L OGISTIC  R EGRESSION \n",
            "Processed page 101 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 102 of 528.\n",
            "First Line :\n",
            "106 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 103 of 528.\n",
            "First Line :\n",
            "6.1 • L EXICAL  S EMANTICS 107 \n",
            "Processed page 104 of 528.\n",
            "First Line :\n",
            "108 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 105 of 528.\n",
            "First Line :\n",
            "6.2 • V ECTOR  S EMANTICS 109 \n",
            "Processed page 106 of 528.\n",
            "First Line :\n",
            "110 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 107 of 528.\n",
            "First Line :\n",
            "6.3 • W ORDS AND  V ECTORS 111 \n",
            "Processed page 108 of 528.\n",
            "First Line :\n",
            "112 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 109 of 528.\n",
            "First Line :\n",
            "6.3 • W ORDS AND  V ECTORS 113 \n",
            "Processed page 110 of 528.\n",
            "First Line :\n",
            "114 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 111 of 528.\n",
            "First Line :\n",
            "6.5 • TF-IDF: W EIGHING TERMS IN THE VECTOR 115 \n",
            "Processed page 112 of 528.\n",
            "First Line :\n",
            "116 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 113 of 528.\n",
            "First Line :\n",
            "6.5 • TF-IDF: W EIGHING TERMS IN THE VECTOR 117 \n",
            "Processed page 114 of 528.\n",
            "First Line :\n",
            "118 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 115 of 528.\n",
            "First Line :\n",
            "6.6 • P OINTWISE  M UTUAL  I NFORMATION  (PMI) 119 \n",
            "Processed page 116 of 528.\n",
            "First Line :\n",
            "120 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 117 of 528.\n",
            "First Line :\n",
            "6.8 • W ORD 2 VEC 121 \n",
            "Processed page 118 of 528.\n",
            "First Line :\n",
            "122 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 119 of 528.\n",
            "First Line :\n",
            "6.8 • W ORD 2 VEC 123 \n",
            "Processed page 120 of 528.\n",
            "First Line :\n",
            "124 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 121 of 528.\n",
            "First Line :\n",
            "6.8 • W ORD 2 VEC 125 \n",
            "Processed page 122 of 528.\n",
            "First Line :\n",
            "126 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 123 of 528.\n",
            "First Line :\n",
            "6.9 • V ISUALIZING  E MBEDDINGS 127 \n",
            "Processed page 124 of 528.\n",
            "First Line :\n",
            "128 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 125 of 528.\n",
            "First Line :\n",
            "6.10 • S EMANTIC PROPERTIES OF EMBEDDINGS 129 \n",
            "Processed page 126 of 528.\n",
            "First Line :\n",
            "130 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 127 of 528.\n",
            "First Line :\n",
            "6.12 • E VALUATING  V ECTOR  M ODELS 131 \n",
            "Processed page 128 of 528.\n",
            "First Line :\n",
            "132 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 129 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 133 \n",
            "Processed page 130 of 528.\n",
            "First Line :\n",
            "134 C HAPTER  6 • V ECTOR  S EMANTICS AND  E MBEDDINGS \n",
            "Processed page 131 of 528.\n",
            "First Line :\n",
            "E XERCISES 135 \n",
            "Processed page 132 of 528.\n",
            "First Line :\n",
            "136 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 133 of 528.\n",
            "First Line :\n",
            "7.1 • U NITS 137 \n",
            "Processed page 134 of 528.\n",
            "First Line :\n",
            "138 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 135 of 528.\n",
            "First Line :\n",
            "7.2 • T HE  XOR  PROBLEM 139 \n",
            "Processed page 136 of 528.\n",
            "First Line :\n",
            "140 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 137 of 528.\n",
            "First Line :\n",
            "7.2 • T HE  XOR  PROBLEM 141 \n",
            "Processed page 138 of 528.\n",
            "First Line :\n",
            "142 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 139 of 528.\n",
            "First Line :\n",
            "7.3 • F EEDFORWARD  N EURAL  N ETWORKS 143 \n",
            "Processed page 140 of 528.\n",
            "First Line :\n",
            "144 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 141 of 528.\n",
            "First Line :\n",
            "7.3 • F EEDFORWARD  N EURAL  N ETWORKS 145 \n",
            "Processed page 142 of 528.\n",
            "First Line :\n",
            "146 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 143 of 528.\n",
            "First Line :\n",
            "7.4 • F EEDFORWARD NETWORKS FOR  NLP: C LASSIFICATION 147 \n",
            "Processed page 144 of 528.\n",
            "First Line :\n",
            "148 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 145 of 528.\n",
            "First Line :\n",
            "7.5 • T RAINING  N EURAL  N ETS 149 \n",
            "Processed page 146 of 528.\n",
            "First Line :\n",
            "150 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 147 of 528.\n",
            "First Line :\n",
            "7.5 • T RAINING  N EURAL  N ETS 151 \n",
            "Processed page 148 of 528.\n",
            "First Line :\n",
            "152 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 149 of 528.\n",
            "First Line :\n",
            "7.5 • T RAINING  N EURAL  N ETS 153 \n",
            "Processed page 150 of 528.\n",
            "First Line :\n",
            "154 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 151 of 528.\n",
            "First Line :\n",
            "7.5 • T RAINING  N EURAL  N ETS 155 \n",
            "Processed page 152 of 528.\n",
            "First Line :\n",
            "156 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 153 of 528.\n",
            "First Line :\n",
            "7.6 • F EEDFORWARD  N EURAL  L ANGUAGE  M ODELING 157 \n",
            "Processed page 154 of 528.\n",
            "First Line :\n",
            "158 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 155 of 528.\n",
            "First Line :\n",
            "7.7 • T RAINING THE NEURAL LANGUAGE MODEL 159 \n",
            "Processed page 156 of 528.\n",
            "First Line :\n",
            "160 C HAPTER  7 • N EURAL  N ETWORKS AND  N EURAL  L ANGUAGE  M ODELS \n",
            "Processed page 157 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 161 \n",
            "Processed page 158 of 528.\n",
            "First Line :\n",
            "162 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 159 of 528.\n",
            "First Line :\n",
            "8.1 • (M OSTLY ) E NGLISH  W ORD  C LASSES 163 \n",
            "Processed page 160 of 528.\n",
            "First Line :\n",
            "164 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 161 of 528.\n",
            "First Line :\n",
            "8.2 • P ART - OF -S PEECH  T AGGING 165 \n",
            "Processed page 162 of 528.\n",
            "First Line :\n",
            "166 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 163 of 528.\n",
            "First Line :\n",
            "8.3 • N AMED  E NTITIES AND  N AMED  E NTITY  T AGGING 167 \n",
            "Processed page 164 of 528.\n",
            "First Line :\n",
            "168 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 165 of 528.\n",
            "First Line :\n",
            "8.4 • HMM P ART - OF -S PEECH  T AGGING 169 \n",
            "Processed page 166 of 528.\n",
            "First Line :\n",
            "170 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 167 of 528.\n",
            "First Line :\n",
            "8.4 • HMM P ART - OF -S PEECH  T AGGING 171 \n",
            "Processed page 168 of 528.\n",
            "First Line :\n",
            "172 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 169 of 528.\n",
            "First Line :\n",
            "8.4 • HMM P ART - OF -S PEECH  T AGGING 173 \n",
            "Processed page 170 of 528.\n",
            "First Line :\n",
            "174 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 171 of 528.\n",
            "First Line :\n",
            "8.4 • HMM P ART - OF -S PEECH  T AGGING 175 \n",
            "Processed page 172 of 528.\n",
            "First Line :\n",
            "176 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 173 of 528.\n",
            "First Line :\n",
            "8.5 • C ONDITIONAL  R ANDOM  F IELDS  (CRF S ) 177 \n",
            "Processed page 174 of 528.\n",
            "First Line :\n",
            "178 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 175 of 528.\n",
            "First Line :\n",
            "8.5 • C ONDITIONAL  R ANDOM  F IELDS  (CRF S ) 179 \n",
            "Processed page 176 of 528.\n",
            "First Line :\n",
            "180 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 177 of 528.\n",
            "First Line :\n",
            "8.6 • E VALUATION OF  N AMED  E NTITY  R ECOGNITION 181 \n",
            "Processed page 178 of 528.\n",
            "First Line :\n",
            "182 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 179 of 528.\n",
            "First Line :\n",
            "8.8 • S UMMARY 183 \n",
            "Processed page 180 of 528.\n",
            "First Line :\n",
            "184 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 181 of 528.\n",
            "First Line :\n",
            "E XERCISES 185 \n",
            "Processed page 182 of 528.\n",
            "First Line :\n",
            "186 C HAPTER  8 • S EQUENCE  L ABELING FOR  P ARTS OF  S PEECH AND  N AMED  E NTITIES \n",
            "Processed page 183 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 184 of 528.\n",
            "First Line :\n",
            "188 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 185 of 528.\n",
            "First Line :\n",
            "9.1 • R ECURRENT  N EURAL  N ETWORKS 189 \n",
            "Processed page 186 of 528.\n",
            "First Line :\n",
            "190 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 187 of 528.\n",
            "First Line :\n",
            "9.2 • RNN S AS  L ANGUAGE  M ODELS 191 \n",
            "Processed page 188 of 528.\n",
            "First Line :\n",
            "192 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 189 of 528.\n",
            "First Line :\n",
            "9.2 • RNN S AS  L ANGUAGE  M ODELS 193 \n",
            "Processed page 190 of 528.\n",
            "First Line :\n",
            "194 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 191 of 528.\n",
            "First Line :\n",
            "9.3 • RNN S FOR OTHER  NLP  TASKS 195 \n",
            "Processed page 192 of 528.\n",
            "First Line :\n",
            "196 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 193 of 528.\n",
            "First Line :\n",
            "9.4 • S TACKED AND  B IDIRECTIONAL  RNN  ARCHITECTURES 197 \n",
            "Processed page 194 of 528.\n",
            "First Line :\n",
            "198 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 195 of 528.\n",
            "First Line :\n",
            "9.4 • S TACKED AND  B IDIRECTIONAL  RNN  ARCHITECTURES 199 \n",
            "Processed page 196 of 528.\n",
            "First Line :\n",
            "200 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 197 of 528.\n",
            "First Line :\n",
            "9.5 • T HE  LSTM 201 \n",
            "Processed page 198 of 528.\n",
            "First Line :\n",
            "202 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 199 of 528.\n",
            "First Line :\n",
            "9.6 • S UMMARY : C OMMON  RNN NLP A RCHITECTURES 203 \n",
            "Processed page 200 of 528.\n",
            "First Line :\n",
            "204 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 201 of 528.\n",
            "First Line :\n",
            "9.7 • T HE  E NCODER -D ECODER  M ODEL WITH  RNN S 205 \n",
            "Processed page 202 of 528.\n",
            "First Line :\n",
            "206 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 203 of 528.\n",
            "First Line :\n",
            "9.7 • T HE  E NCODER -D ECODER  M ODEL WITH  RNN S 207 \n",
            "Processed page 204 of 528.\n",
            "First Line :\n",
            "208 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 205 of 528.\n",
            "First Line :\n",
            "9.8 • A TTENTION 209 \n",
            "Processed page 206 of 528.\n",
            "First Line :\n",
            "210 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 207 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 211 \n",
            "Processed page 208 of 528.\n",
            "First Line :\n",
            "212 C HAPTER  9 • RNN S AND  LSTM S \n",
            "Processed page 209 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 210 of 528.\n",
            "First Line :\n",
            "214 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 211 of 528.\n",
            "First Line :\n",
            "10.1 • T HE  T RANSFORMER : A S ELF -A TTENTION  N ETWORK 215 \n",
            "Processed page 212 of 528.\n",
            "First Line :\n",
            "216 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 213 of 528.\n",
            "First Line :\n",
            "10.1 • T HE  T RANSFORMER : A S ELF -A TTENTION  N ETWORK 217 \n",
            "Processed page 214 of 528.\n",
            "First Line :\n",
            "218 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 215 of 528.\n",
            "First Line :\n",
            "10.1 • T HE  T RANSFORMER : A S ELF -A TTENTION  N ETWORK 219 \n",
            "Processed page 216 of 528.\n",
            "First Line :\n",
            "220 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 217 of 528.\n",
            "First Line :\n",
            "10.2 • M ULTIHEAD  A TTENTION 221 \n",
            "Processed page 218 of 528.\n",
            "First Line :\n",
            "222 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 219 of 528.\n",
            "First Line :\n",
            "10.3 • T RANSFORMER  B LOCKS 223 \n",
            "Processed page 220 of 528.\n",
            "First Line :\n",
            "224 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 221 of 528.\n",
            "First Line :\n",
            "10.4 • T HE  R ESIDUAL  S TREAM VIEW OF THE  T RANSFORMER  B LOCK 225 \n",
            "Processed page 222 of 528.\n",
            "First Line :\n",
            "226 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 223 of 528.\n",
            "First Line :\n",
            "10.5 • T HE INPUT :  EMBEDDINGS FOR TOKEN AND POSITION 227 \n",
            "Processed page 224 of 528.\n",
            "First Line :\n",
            "228 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 225 of 528.\n",
            "First Line :\n",
            "10.6 • T HE  L ANGUAGE  M ODELING  H EAD 229 \n",
            "Processed page 226 of 528.\n",
            "First Line :\n",
            "230 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 227 of 528.\n",
            "First Line :\n",
            "10.7 • L ARGE  L ANGUAGE  M ODELS WITH  T RANSFORMERS 231 \n",
            "Processed page 228 of 528.\n",
            "First Line :\n",
            "232 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 229 of 528.\n",
            "First Line :\n",
            "10.7 • L ARGE  L ANGUAGE  M ODELS WITH  T RANSFORMERS 233 \n",
            "Processed page 230 of 528.\n",
            "First Line :\n",
            "234 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 231 of 528.\n",
            "First Line :\n",
            "10.8 • L ARGE  L ANGUAGE  M ODELS : G ENERATION BY  S AMPLING 235 \n",
            "Processed page 232 of 528.\n",
            "First Line :\n",
            "236 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 233 of 528.\n",
            "First Line :\n",
            "10.9 • L ARGE  L ANGUAGE  M ODELS : T RAINING  T RANSFORMERS 237 \n",
            "Processed page 234 of 528.\n",
            "First Line :\n",
            "238 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 235 of 528.\n",
            "First Line :\n",
            "10.10 • P OTENTIAL  H ARMS FROM  L ANGUAGE  M ODELS 239 \n",
            "Processed page 236 of 528.\n",
            "First Line :\n",
            "240 C HAPTER  10 • T RANSFORMERS AND  L ARGE  L ANGUAGE  M ODELS \n",
            "Processed page 237 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 241 \n",
            "Processed page 238 of 528.\n",
            "First Line :\n",
            "242 C HAPTER  11 • F INE -T UNING AND  M ASKED  L ANGUAGE  M ODELS \n",
            "Processed page 239 of 528.\n",
            "First Line :\n",
            "11.1 • B IDIRECTIONAL  T RANSFORMER  E NCODERS 243 \n",
            "Processed page 240 of 528.\n",
            "First Line :\n",
            "244 C HAPTER  11 • F INE -T UNING AND  M ASKED  L ANGUAGE  M ODELS \n",
            "Processed page 241 of 528.\n",
            "First Line :\n",
            "11.1 • B IDIRECTIONAL  T RANSFORMER  E NCODERS 245 \n",
            "Processed page 242 of 528.\n",
            "First Line :\n",
            "246 C HAPTER  11 • F INE -T UNING AND  M ASKED  L ANGUAGE  M ODELS \n",
            "Processed page 243 of 528.\n",
            "First Line :\n",
            "11.2 • T RAINING  B IDIRECTIONAL  E NCODERS 247 \n",
            "Processed page 244 of 528.\n",
            "First Line :\n",
            "248 C HAPTER  11 • F INE -T UNING AND  M ASKED  L ANGUAGE  M ODELS \n",
            "Processed page 245 of 528.\n",
            "First Line :\n",
            "11.2 • T RAINING  B IDIRECTIONAL  E NCODERS 249 \n",
            "Processed page 246 of 528.\n",
            "First Line :\n",
            "250 C HAPTER  11 • F INE -T UNING AND  M ASKED  L ANGUAGE  M ODELS \n",
            "Processed page 247 of 528.\n",
            "First Line :\n",
            "11.3 • C ONTEXTUAL  E MBEDDINGS 251 \n",
            "Processed page 248 of 528.\n",
            "First Line :\n",
            "252 C HAPTER  11 • F INE -T UNING AND  M ASKED  L ANGUAGE  M ODELS \n",
            "Processed page 249 of 528.\n",
            "First Line :\n",
            "11.3 • C ONTEXTUAL  E MBEDDINGS 253 \n",
            "Processed page 250 of 528.\n",
            "First Line :\n",
            "254 C HAPTER  11 • F INE -T UNING AND  M ASKED  L ANGUAGE  M ODELS \n",
            "Processed page 251 of 528.\n",
            "First Line :\n",
            "11.4 • F INE -T UNING  L ANGUAGE  M ODELS 255 \n",
            "Processed page 252 of 528.\n",
            "First Line :\n",
            "256 C HAPTER  11 • F INE -T UNING AND  M ASKED  L ANGUAGE  M ODELS \n",
            "Processed page 253 of 528.\n",
            "First Line :\n",
            "11.4 • F INE -T UNING  L ANGUAGE  M ODELS 257 \n",
            "Processed page 254 of 528.\n",
            "First Line :\n",
            "258 C HAPTER  11 • F INE -T UNING AND  M ASKED  L ANGUAGE  M ODELS \n",
            "Processed page 255 of 528.\n",
            "First Line :\n",
            "11.5 • A DVANCED : S PAN - BASED  M ASKING 259 \n",
            "Processed page 256 of 528.\n",
            "First Line :\n",
            "260 C HAPTER  11 • F INE -T UNING AND  M ASKED  L ANGUAGE  M ODELS \n",
            "Processed page 257 of 528.\n",
            "First Line :\n",
            "11.5 • A DVANCED : S PAN - BASED  M ASKING 261 \n",
            "Processed page 258 of 528.\n",
            "First Line :\n",
            "262 C HAPTER  11 • F INE -T UNING AND  M ASKED  L ANGUAGE  M ODELS \n",
            "Processed page 259 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 260 of 528.\n",
            "Processed page 261 of 528.\n",
            "First Line :\n",
            "# Part II\n",
            "\n",
            "\n",
            "Processed page 262 of 528.\n",
            "Processed page 263 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 264 of 528.\n",
            "First Line :\n",
            "268 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 265 of 528.\n",
            "First Line :\n",
            "13.1 • L ANGUAGE  D IVERGENCES AND  T YPOLOGY 269 \n",
            "Processed page 266 of 528.\n",
            "First Line :\n",
            "270 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 267 of 528.\n",
            "First Line :\n",
            "13.1 • L ANGUAGE  D IVERGENCES AND  T YPOLOGY 271 \n",
            "Processed page 268 of 528.\n",
            "First Line :\n",
            "272 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 269 of 528.\n",
            "First Line :\n",
            "13.2 • M ACHINE  T RANSLATION USING  E NCODER -D ECODER 273 \n",
            "Processed page 270 of 528.\n",
            "First Line :\n",
            "274 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 271 of 528.\n",
            "First Line :\n",
            "13.2 • M ACHINE  T RANSLATION USING  E NCODER -D ECODER 275 \n",
            "Processed page 272 of 528.\n",
            "First Line :\n",
            "276 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 273 of 528.\n",
            "First Line :\n",
            "13.3 • D ETAILS OF THE  E NCODER -D ECODER  M ODEL 277 \n",
            "Processed page 274 of 528.\n",
            "First Line :\n",
            "278 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 275 of 528.\n",
            "First Line :\n",
            "13.4 • D ECODING IN  MT: B EAM  S EARCH 279 \n",
            "Processed page 276 of 528.\n",
            "First Line :\n",
            "280 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 277 of 528.\n",
            "First Line :\n",
            "13.4 • D ECODING IN  MT: B EAM  S EARCH 281 \n",
            "Processed page 278 of 528.\n",
            "First Line :\n",
            "282 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 279 of 528.\n",
            "First Line :\n",
            "13.5 • T RANSLATING IN LOW - RESOURCE SITUATIONS 283 \n",
            "Processed page 280 of 528.\n",
            "First Line :\n",
            "284 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 281 of 528.\n",
            "First Line :\n",
            "13.6 • MT E VALUATION 285 \n",
            "Processed page 282 of 528.\n",
            "First Line :\n",
            "286 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 283 of 528.\n",
            "First Line :\n",
            "13.6 • MT E VALUATION 287 \n",
            "Processed page 284 of 528.\n",
            "First Line :\n",
            "288 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 285 of 528.\n",
            "First Line :\n",
            "13.8 • S UMMARY 289 \n",
            "Processed page 286 of 528.\n",
            "First Line :\n",
            "290 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 287 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 291 \n",
            "Processed page 288 of 528.\n",
            "First Line :\n",
            "292 C HAPTER  13 • M ACHINE  T RANSLATION \n",
            "Processed page 289 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 290 of 528.\n",
            "First Line :\n",
            "294 C HAPTER  14 • Q UESTION  A NSWERING AND  I NFORMATION  R ETRIEVAL \n",
            "Processed page 291 of 528.\n",
            "First Line :\n",
            "14.1 • I NFORMATION  R ETRIEVAL 295 \n",
            "Processed page 292 of 528.\n",
            "First Line :\n",
            "296 C HAPTER  14 • Q UESTION  A NSWERING AND  I NFORMATION  R ETRIEVAL \n",
            "Processed page 293 of 528.\n",
            "First Line :\n",
            "14.1 • I NFORMATION  R ETRIEVAL 297 \n",
            "Processed page 294 of 528.\n",
            "First Line :\n",
            "298 C HAPTER  14 • Q UESTION  A NSWERING AND  I NFORMATION  R ETRIEVAL \n",
            "Processed page 295 of 528.\n",
            "First Line :\n",
            "14.1 • I NFORMATION  R ETRIEVAL 299 \n",
            "Processed page 296 of 528.\n",
            "First Line :\n",
            "300 C HAPTER  14 • Q UESTION  A NSWERING AND  I NFORMATION  R ETRIEVAL \n",
            "Processed page 297 of 528.\n",
            "First Line :\n",
            "14.1 • I NFORMATION  R ETRIEVAL 301 \n",
            "Processed page 298 of 528.\n",
            "First Line :\n",
            "302 C HAPTER  14 • Q UESTION  A NSWERING AND  I NFORMATION  R ETRIEVAL \n",
            "Processed page 299 of 528.\n",
            "First Line :\n",
            "14.2 • I NFORMATION  R ETRIEVAL WITH  D ENSE  V ECTORS 303 \n",
            "Processed page 300 of 528.\n",
            "First Line :\n",
            "304 C HAPTER  14 • Q UESTION  A NSWERING AND  I NFORMATION  R ETRIEVAL \n",
            "Processed page 301 of 528.\n",
            "First Line :\n",
            "14.3 • U SING  N EURAL  IR  FOR  Q UESTION  A NSWERING 305 \n",
            "Processed page 302 of 528.\n",
            "First Line :\n",
            "306 C HAPTER  14 • Q UESTION  A NSWERING AND  I NFORMATION  R ETRIEVAL \n",
            "Processed page 303 of 528.\n",
            "First Line :\n",
            "14.3 • U SING  N EURAL  IR  FOR  Q UESTION  A NSWERING 307 \n",
            "Processed page 304 of 528.\n",
            "First Line :\n",
            "308 C HAPTER  14 • Q UESTION  A NSWERING AND  I NFORMATION  R ETRIEVAL \n",
            "Processed page 305 of 528.\n",
            "First Line :\n",
            "14.3 • U SING  N EURAL  IR  FOR  Q UESTION  A NSWERING 309 \n",
            "Processed page 306 of 528.\n",
            "First Line :\n",
            "310 C HAPTER  14 • Q UESTION  A NSWERING AND  I NFORMATION  R ETRIEVAL \n",
            "Processed page 307 of 528.\n",
            "First Line :\n",
            "14.4 • E VALUATING  R ETRIEVAL - BASED  Q UESTION  A NSWERING 311 \n",
            "Processed page 308 of 528.\n",
            "First Line :\n",
            "312 C HAPTER  14 • Q UESTION  A NSWERING AND  I NFORMATION  R ETRIEVAL \n",
            "Processed page 309 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 313 \n",
            "Processed page 310 of 528.\n",
            "First Line :\n",
            "314 C HAPTER  14 • Q UESTION  A NSWERING AND  I NFORMATION  R ETRIEVAL \n",
            "Processed page 311 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 312 of 528.\n",
            "First Line :\n",
            "316 C HAPTER  15 • C HATBOTS  & D IALOGUE  S YSTEMS \n",
            "Processed page 313 of 528.\n",
            "First Line :\n",
            "15.1 • P ROPERTIES OF  H UMAN  C ONVERSATION 317 \n",
            "Processed page 314 of 528.\n",
            "First Line :\n",
            "318 C HAPTER  15 • C HATBOTS  & D IALOGUE  S YSTEMS \n",
            "Processed page 315 of 528.\n",
            "First Line :\n",
            "15.1 • P ROPERTIES OF  H UMAN  C ONVERSATION 319 \n",
            "Processed page 316 of 528.\n",
            "First Line :\n",
            "320 C HAPTER  15 • C HATBOTS  & D IALOGUE  S YSTEMS \n",
            "Processed page 317 of 528.\n",
            "First Line :\n",
            "15.2 • F RAME -B ASED  D IALOGUE  S YSTEMS 321 \n",
            "Processed page 318 of 528.\n",
            "First Line :\n",
            "322 C HAPTER  15 • C HATBOTS  & D IALOGUE  S YSTEMS \n",
            "Processed page 319 of 528.\n",
            "First Line :\n",
            "15.3 • D IALOGUE  A CTS AND  D IALOGUE  S TATE 323 \n",
            "Processed page 320 of 528.\n",
            "First Line :\n",
            "324 C HAPTER  15 • C HATBOTS  & D IALOGUE  S YSTEMS \n",
            "Processed page 321 of 528.\n",
            "First Line :\n",
            "15.3 • D IALOGUE  A CTS AND  D IALOGUE  S TATE 325 \n",
            "Processed page 322 of 528.\n",
            "First Line :\n",
            "326 C HAPTER  15 • C HATBOTS  & D IALOGUE  S YSTEMS \n",
            "Processed page 323 of 528.\n",
            "First Line :\n",
            "15.4 • C HATBOTS 327 \n",
            "Processed page 324 of 528.\n",
            "First Line :\n",
            "328 C HAPTER  15 • C HATBOTS  & D IALOGUE  S YSTEMS \n",
            "Processed page 325 of 528.\n",
            "First Line :\n",
            "15.4 • C HATBOTS 329 \n",
            "Processed page 326 of 528.\n",
            "First Line :\n",
            "330 C HAPTER  15 • C HATBOTS  & D IALOGUE  S YSTEMS \n",
            "Processed page 327 of 528.\n",
            "First Line :\n",
            "15.5 • D IALOGUE  S YSTEM  D ESIGN 331 \n",
            "Processed page 328 of 528.\n",
            "First Line :\n",
            "332 C HAPTER  15 • C HATBOTS  & D IALOGUE  S YSTEMS \n",
            "Processed page 329 of 528.\n",
            "First Line :\n",
            "15.6 • S UMMARY 333 \n",
            "Processed page 330 of 528.\n",
            "First Line :\n",
            "334 C HAPTER  15 • C HATBOTS  & D IALOGUE  S YSTEMS \n",
            "Processed page 331 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 335 \n",
            "Processed page 332 of 528.\n",
            "First Line :\n",
            "336 C HAPTER  15 • C HATBOTS  & D IALOGUE  S YSTEMS \n",
            "Processed page 333 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 334 of 528.\n",
            "First Line :\n",
            "338 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 335 of 528.\n",
            "First Line :\n",
            "16.1 • T HE  A UTOMATIC  S PEECH  R ECOGNITION  T ASK 339 \n",
            "Processed page 336 of 528.\n",
            "First Line :\n",
            "340 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 337 of 528.\n",
            "First Line :\n",
            "16.2 • F EATURE  E XTRACTION FOR  ASR: L OG  M EL  S PECTRUM 341 \n",
            "Processed page 338 of 528.\n",
            "First Line :\n",
            "342 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 339 of 528.\n",
            "First Line :\n",
            "16.2 • F EATURE  E XTRACTION FOR  ASR: L OG  M EL  S PECTRUM 343 \n",
            "Processed page 340 of 528.\n",
            "First Line :\n",
            "344 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 341 of 528.\n",
            "First Line :\n",
            "16.3 • S PEECH  R ECOGNITION  A RCHITECTURE 345 \n",
            "Processed page 342 of 528.\n",
            "First Line :\n",
            "346 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 343 of 528.\n",
            "First Line :\n",
            "16.4 • CTC 347 \n",
            "Processed page 344 of 528.\n",
            "First Line :\n",
            "348 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 345 of 528.\n",
            "First Line :\n",
            "16.4 • CTC 349 \n",
            "Processed page 346 of 528.\n",
            "First Line :\n",
            "350 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 347 of 528.\n",
            "First Line :\n",
            "16.4 • CTC 351 \n",
            "Processed page 348 of 528.\n",
            "First Line :\n",
            "352 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 349 of 528.\n",
            "First Line :\n",
            "16.5 • ASR E VALUATION : W ORD  E RROR  R ATE 353 \n",
            "Processed page 350 of 528.\n",
            "First Line :\n",
            "354 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 351 of 528.\n",
            "First Line :\n",
            "16.6 • TTS 355 \n",
            "Processed page 352 of 528.\n",
            "First Line :\n",
            "356 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 353 of 528.\n",
            "First Line :\n",
            "16.6 • TTS 357 \n",
            "Processed page 354 of 528.\n",
            "First Line :\n",
            "358 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 355 of 528.\n",
            "First Line :\n",
            "16.7 • O THER  S PEECH  T ASKS 359 \n",
            "Processed page 356 of 528.\n",
            "First Line :\n",
            "360 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 357 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 361 \n",
            "Processed page 358 of 528.\n",
            "First Line :\n",
            "362 C HAPTER  16 • A UTOMATIC  S PEECH  R ECOGNITION AND  T EXT - TO -S PEECH \n",
            "Processed page 359 of 528.\n",
            "First Line :\n",
            "E XERCISES 363 \n",
            "Processed page 360 of 528.\n",
            "Processed page 361 of 528.\n",
            "First Line :\n",
            "# Part III\n",
            "\n",
            "\n",
            "Processed page 362 of 528.\n",
            "Processed page 363 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 364 of 528.\n",
            "First Line :\n",
            "368 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 365 of 528.\n",
            "First Line :\n",
            "17.2 • C ONTEXT -F REE  G RAMMARS 369 \n",
            "Processed page 366 of 528.\n",
            "First Line :\n",
            "370 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 367 of 528.\n",
            "First Line :\n",
            "17.2 • C ONTEXT -F REE  G RAMMARS 371 \n",
            "Processed page 368 of 528.\n",
            "First Line :\n",
            "372 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 369 of 528.\n",
            "First Line :\n",
            "17.3 • T REEBANKS 373 \n",
            "Processed page 370 of 528.\n",
            "First Line :\n",
            "374 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 371 of 528.\n",
            "First Line :\n",
            "17.5 • A MBIGUITY 375 \n",
            "Processed page 372 of 528.\n",
            "First Line :\n",
            "376 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 373 of 528.\n",
            "First Line :\n",
            "17.6 • CKY P ARSING : A D YNAMIC  P ROGRAMMING  A PPROACH 377 \n",
            "Processed page 374 of 528.\n",
            "First Line :\n",
            "378 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 375 of 528.\n",
            "First Line :\n",
            "17.6 • CKY P ARSING : A D YNAMIC  P ROGRAMMING  A PPROACH 379 \n",
            "Processed page 376 of 528.\n",
            "First Line :\n",
            "380 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 377 of 528.\n",
            "First Line :\n",
            "17.6 • CKY P ARSING : A D YNAMIC  P ROGRAMMING  A PPROACH 381 \n",
            "Processed page 378 of 528.\n",
            "First Line :\n",
            "382 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 379 of 528.\n",
            "First Line :\n",
            "17.7 • S PAN -B ASED  N EURAL  C ONSTITUENCY  P ARSING 383 \n",
            "Processed page 380 of 528.\n",
            "First Line :\n",
            "384 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 381 of 528.\n",
            "First Line :\n",
            "17.8 • E VALUATING  P ARSERS 385 \n",
            "Processed page 382 of 528.\n",
            "First Line :\n",
            "386 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 383 of 528.\n",
            "First Line :\n",
            "17.10 • S UMMARY 387 \n",
            "Processed page 384 of 528.\n",
            "First Line :\n",
            "388 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 385 of 528.\n",
            "First Line :\n",
            "E XERCISES 389 \n",
            "Processed page 386 of 528.\n",
            "First Line :\n",
            "390 C HAPTER  17 • C ONTEXT -F REE  G RAMMARS AND  C ONSTITUENCY  P ARSING \n",
            "Processed page 387 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 388 of 528.\n",
            "First Line :\n",
            "392 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 389 of 528.\n",
            "First Line :\n",
            "18.1 • D EPENDENCY  R ELATIONS 393 \n",
            "Processed page 390 of 528.\n",
            "First Line :\n",
            "394 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 391 of 528.\n",
            "First Line :\n",
            "18.1 • D EPENDENCY  R ELATIONS 395 \n",
            "Processed page 392 of 528.\n",
            "First Line :\n",
            "396 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 393 of 528.\n",
            "First Line :\n",
            "18.2 • T RANSITION -B ASED  D EPENDENCY  P ARSING 397 \n",
            "Processed page 394 of 528.\n",
            "First Line :\n",
            "398 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 395 of 528.\n",
            "First Line :\n",
            "18.2 • T RANSITION -B ASED  D EPENDENCY  P ARSING 399 \n",
            "Processed page 396 of 528.\n",
            "First Line :\n",
            "400 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 397 of 528.\n",
            "First Line :\n",
            "18.2 • T RANSITION -B ASED  D EPENDENCY  P ARSING 401 \n",
            "Processed page 398 of 528.\n",
            "First Line :\n",
            "402 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 399 of 528.\n",
            "First Line :\n",
            "18.2 • T RANSITION -B ASED  D EPENDENCY  P ARSING 403 \n",
            "Processed page 400 of 528.\n",
            "First Line :\n",
            "404 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 401 of 528.\n",
            "First Line :\n",
            "18.3 • G RAPH -B ASED  D EPENDENCY  P ARSING 405 \n",
            "Processed page 402 of 528.\n",
            "First Line :\n",
            "406 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 403 of 528.\n",
            "First Line :\n",
            "18.3 • G RAPH -B ASED  D EPENDENCY  P ARSING 407 \n",
            "Processed page 404 of 528.\n",
            "First Line :\n",
            "408 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 405 of 528.\n",
            "First Line :\n",
            "18.3 • G RAPH -B ASED  D EPENDENCY  P ARSING 409 \n",
            "Processed page 406 of 528.\n",
            "First Line :\n",
            "410 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 407 of 528.\n",
            "First Line :\n",
            "18.4 • E VALUATION 411 \n",
            "Processed page 408 of 528.\n",
            "First Line :\n",
            "412 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 409 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 413 \n",
            "Processed page 410 of 528.\n",
            "First Line :\n",
            "414 C HAPTER  18 • D EPENDENCY  P ARSING \n",
            "Processed page 411 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 412 of 528.\n",
            "First Line :\n",
            "416 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 413 of 528.\n",
            "First Line :\n",
            "19.1 • R ELATION  E XTRACTION 417 \n",
            "Processed page 414 of 528.\n",
            "First Line :\n",
            "418 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 415 of 528.\n",
            "First Line :\n",
            "19.2 • R ELATION  E XTRACTION  A LGORITHMS 419 \n",
            "Processed page 416 of 528.\n",
            "First Line :\n",
            "420 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 417 of 528.\n",
            "First Line :\n",
            "19.2 • R ELATION  E XTRACTION  A LGORITHMS 421 \n",
            "Processed page 418 of 528.\n",
            "First Line :\n",
            "422 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 419 of 528.\n",
            "First Line :\n",
            "19.2 • R ELATION  E XTRACTION  A LGORITHMS 423 \n",
            "Processed page 420 of 528.\n",
            "First Line :\n",
            "424 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 421 of 528.\n",
            "First Line :\n",
            "19.2 • R ELATION  E XTRACTION  A LGORITHMS 425 \n",
            "Processed page 422 of 528.\n",
            "First Line :\n",
            "426 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 423 of 528.\n",
            "First Line :\n",
            "19.4 • R EPRESENTING  T IME 427 \n",
            "Processed page 424 of 528.\n",
            "First Line :\n",
            "428 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 425 of 528.\n",
            "First Line :\n",
            "19.4 • R EPRESENTING  T IME 429 \n",
            "Processed page 426 of 528.\n",
            "First Line :\n",
            "430 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 427 of 528.\n",
            "First Line :\n",
            "19.6 • T EMPORALLY  A NNOTATED  D ATASETS : T IME B ANK 431 \n",
            "Processed page 428 of 528.\n",
            "First Line :\n",
            "432 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 429 of 528.\n",
            "First Line :\n",
            "19.7 • A UTOMATIC  T EMPORAL  A NALYSIS 433 \n",
            "Processed page 430 of 528.\n",
            "First Line :\n",
            "434 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 431 of 528.\n",
            "First Line :\n",
            "19.7 • A UTOMATIC  T EMPORAL  A NALYSIS 435 \n",
            "Processed page 432 of 528.\n",
            "First Line :\n",
            "436 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 433 of 528.\n",
            "First Line :\n",
            "19.8 • T EMPLATE  F ILLING 437 \n",
            "Processed page 434 of 528.\n",
            "First Line :\n",
            "438 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 435 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 439 \n",
            "Processed page 436 of 528.\n",
            "First Line :\n",
            "440 C HAPTER  19 • I NFORMATION  E XTRACTION : R ELATIONS , E VENTS ,  AND  T IME \n",
            "Processed page 437 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 438 of 528.\n",
            "First Line :\n",
            "442 C HAPTER  20 • S EMANTIC  R OLE  L ABELING \n",
            "Processed page 439 of 528.\n",
            "First Line :\n",
            "20.2 • D IATHESIS  A LTERNATIONS 443 \n",
            "Processed page 440 of 528.\n",
            "First Line :\n",
            "444 C HAPTER  20 • S EMANTIC  R OLE  L ABELING \n",
            "Processed page 441 of 528.\n",
            "First Line :\n",
            "20.4 • T HE  P ROPOSITION  B ANK 445 \n",
            "Processed page 442 of 528.\n",
            "First Line :\n",
            "446 C HAPTER  20 • S EMANTIC  R OLE  L ABELING \n",
            "Processed page 443 of 528.\n",
            "First Line :\n",
            "20.5 • F RAME N ET 447 \n",
            "Processed page 444 of 528.\n",
            "First Line :\n",
            "448 C HAPTER  20 • S EMANTIC  R OLE  L ABELING \n",
            "Processed page 445 of 528.\n",
            "First Line :\n",
            "20.6 • S EMANTIC  R OLE  L ABELING 449 \n",
            "Processed page 446 of 528.\n",
            "First Line :\n",
            "450 C HAPTER  20 • S EMANTIC  R OLE  L ABELING \n",
            "Processed page 447 of 528.\n",
            "First Line :\n",
            "20.6 • S EMANTIC  R OLE  L ABELING 451 \n",
            "Processed page 448 of 528.\n",
            "First Line :\n",
            "452 C HAPTER  20 • S EMANTIC  R OLE  L ABELING \n",
            "Processed page 449 of 528.\n",
            "First Line :\n",
            "20.7 • S ELECTIONAL  R ESTRICTIONS 453 \n",
            "Processed page 450 of 528.\n",
            "First Line :\n",
            "454 C HAPTER  20 • S EMANTIC  R OLE  L ABELING \n",
            "Processed page 451 of 528.\n",
            "First Line :\n",
            "20.7 • S ELECTIONAL  R ESTRICTIONS 455 \n",
            "Processed page 452 of 528.\n",
            "First Line :\n",
            "456 C HAPTER  20 • S EMANTIC  R OLE  L ABELING \n",
            "Processed page 453 of 528.\n",
            "First Line :\n",
            "20.9 • S UMMARY 457 \n",
            "Processed page 454 of 528.\n",
            "First Line :\n",
            "458 C HAPTER  20 • S EMANTIC  R OLE  L ABELING \n",
            "Processed page 455 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 459 \n",
            "Processed page 456 of 528.\n",
            "First Line :\n",
            "460 C HAPTER  20 • S EMANTIC  R OLE  L ABELING \n",
            "Processed page 457 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 458 of 528.\n",
            "First Line :\n",
            "462 C HAPTER  21 • L EXICONS FOR  S ENTIMENT , A FFECT ,  AND  C ONNOTATION \n",
            "Processed page 459 of 528.\n",
            "First Line :\n",
            "21.1 • D EFINING  E MOTION 463 \n",
            "Processed page 460 of 528.\n",
            "First Line :\n",
            "464 C HAPTER  21 • L EXICONS FOR  S ENTIMENT , A FFECT ,  AND  C ONNOTATION \n",
            "Processed page 461 of 528.\n",
            "First Line :\n",
            "21.3 • C REATING  A FFECT  L EXICONS BY  H UMAN  L ABELING 465 \n",
            "Processed page 462 of 528.\n",
            "First Line :\n",
            "466 C HAPTER  21 • L EXICONS FOR  S ENTIMENT , A FFECT ,  AND  C ONNOTATION \n",
            "Processed page 463 of 528.\n",
            "First Line :\n",
            "21.4 • S EMI - SUPERVISED  I NDUCTION OF  A FFECT  L EXICONS 467 \n",
            "Processed page 464 of 528.\n",
            "First Line :\n",
            "468 C HAPTER  21 • L EXICONS FOR  S ENTIMENT , A FFECT ,  AND  C ONNOTATION \n",
            "Processed page 465 of 528.\n",
            "First Line :\n",
            "21.4 • S EMI - SUPERVISED  I NDUCTION OF  A FFECT  L EXICONS 469 \n",
            "Processed page 466 of 528.\n",
            "First Line :\n",
            "470 C HAPTER  21 • L EXICONS FOR  S ENTIMENT , A FFECT ,  AND  C ONNOTATION \n",
            "Processed page 467 of 528.\n",
            "First Line :\n",
            "21.5 • S UPERVISED  L EARNING OF  W ORD  S ENTIMENT 471 \n",
            "Processed page 468 of 528.\n",
            "First Line :\n",
            "IMDB \n",
            "Processed page 469 of 528.\n",
            "First Line :\n",
            "0.0 \n",
            "Processed page 470 of 528.\n",
            "First Line :\n",
            "474 C HAPTER  21 • L EXICONS FOR  S ENTIMENT , A FFECT ,  AND  C ONNOTATION \n",
            "Processed page 471 of 528.\n",
            "First Line :\n",
            "21.6 • U SING  L EXICONS FOR  S ENTIMENT  R ECOGNITION 475 \n",
            "Processed page 472 of 528.\n",
            "First Line :\n",
            "476 C HAPTER  21 • L EXICONS FOR  S ENTIMENT , A FFECT ,  AND  C ONNOTATION \n",
            "Processed page 473 of 528.\n",
            "First Line :\n",
            "21.8 • L EXICON - BASED METHODS FOR  E NTITY -C ENTRIC  A FFECT 477 \n",
            "Processed page 474 of 528.\n",
            "First Line :\n",
            "478 C HAPTER  21 • L EXICONS FOR  S ENTIMENT , A FFECT ,  AND  C ONNOTATION \n",
            "Processed page 475 of 528.\n",
            "First Line :\n",
            "21.10 • S UMMARY 479 \n",
            "Processed page 476 of 528.\n",
            "First Line :\n",
            "480 C HAPTER  21 • L EXICONS FOR  S ENTIMENT , A FFECT ,  AND  C ONNOTATION \n",
            "Processed page 477 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 478 of 528.\n",
            "First Line :\n",
            "482 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 479 of 528.\n",
            "First Line :\n",
            "483 \n",
            "Processed page 480 of 528.\n",
            "First Line :\n",
            "484 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 481 of 528.\n",
            "First Line :\n",
            "22.1 • C OREFERENCE  P HENOMENA : L INGUISTIC  B ACKGROUND 485 \n",
            "Processed page 482 of 528.\n",
            "First Line :\n",
            "486 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 483 of 528.\n",
            "First Line :\n",
            "22.1 • C OREFERENCE  P HENOMENA : L INGUISTIC  B ACKGROUND 487 \n",
            "Processed page 484 of 528.\n",
            "First Line :\n",
            "488 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 485 of 528.\n",
            "First Line :\n",
            "22.2 • C OREFERENCE  T ASKS AND  D ATASETS 489 \n",
            "Processed page 486 of 528.\n",
            "First Line :\n",
            "490 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 487 of 528.\n",
            "First Line :\n",
            "22.3 • M ENTION  D ETECTION 491 \n",
            "Processed page 488 of 528.\n",
            "First Line :\n",
            "492 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 489 of 528.\n",
            "First Line :\n",
            "22.4 • A RCHITECTURES FOR  C OREFERENCE  A LGORITHMS 493 \n",
            "Processed page 490 of 528.\n",
            "First Line :\n",
            "494 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 491 of 528.\n",
            "First Line :\n",
            "22.5 • C LASSIFIERS USING HAND - BUILT FEATURES 495 \n",
            "Processed page 492 of 528.\n",
            "First Line :\n",
            "496 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 493 of 528.\n",
            "First Line :\n",
            "22.6 • A  NEURAL MENTION - RANKING ALGORITHM 497 \n",
            "Processed page 494 of 528.\n",
            "First Line :\n",
            "498 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 495 of 528.\n",
            "First Line :\n",
            "22.6 • A  NEURAL MENTION - RANKING ALGORITHM 499 \n",
            "Processed page 496 of 528.\n",
            "First Line :\n",
            "500 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 497 of 528.\n",
            "First Line :\n",
            "22.7 • E NTITY  L INKING 501 \n",
            "Processed page 498 of 528.\n",
            "First Line :\n",
            "502 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 499 of 528.\n",
            "First Line :\n",
            "22.7 • E NTITY  L INKING 503 \n",
            "Processed page 500 of 528.\n",
            "First Line :\n",
            "504 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 501 of 528.\n",
            "First Line :\n",
            "22.9 • W INOGRAD  S CHEMA PROBLEMS 505 \n",
            "Processed page 502 of 528.\n",
            "First Line :\n",
            "506 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 503 of 528.\n",
            "First Line :\n",
            "22.11 • S UMMARY 507 \n",
            "Processed page 504 of 528.\n",
            "First Line :\n",
            "508 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 505 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 509 \n",
            "Processed page 506 of 528.\n",
            "First Line :\n",
            "510 C HAPTER  22 • C OREFERENCE  R ESOLUTION AND  E NTITY  L INKING \n",
            "Processed page 507 of 528.\n",
            "First Line :\n",
            "\n",
            "Processed page 508 of 528.\n",
            "First Line :\n",
            "512 C HAPTER  23 • D ISCOURSE  C OHERENCE \n",
            "Processed page 509 of 528.\n",
            "First Line :\n",
            "23.1 • C OHERENCE  R ELATIONS 513 \n",
            "Processed page 510 of 528.\n",
            "First Line :\n",
            "514 C HAPTER  23 • D ISCOURSE  C OHERENCE \n",
            "Processed page 511 of 528.\n",
            "First Line :\n",
            "23.1 • C OHERENCE  R ELATIONS 515 \n",
            "Processed page 512 of 528.\n",
            "First Line :\n",
            "516 C HAPTER  23 • D ISCOURSE  C OHERENCE \n",
            "Processed page 513 of 528.\n",
            "First Line :\n",
            "23.2 • D ISCOURSE  S TRUCTURE  P ARSING 517 \n",
            "Processed page 514 of 528.\n",
            "First Line :\n",
            "518 C HAPTER  23 • D ISCOURSE  C OHERENCE \n",
            "Processed page 515 of 528.\n",
            "First Line :\n",
            "23.2 • D ISCOURSE  S TRUCTURE  P ARSING 519 \n",
            "Processed page 516 of 528.\n",
            "First Line :\n",
            "520 C HAPTER  23 • D ISCOURSE  C OHERENCE \n",
            "Processed page 517 of 528.\n",
            "First Line :\n",
            "23.3 • C ENTERING AND  E NTITY -B ASED  C OHERENCE 521 \n",
            "Processed page 518 of 528.\n",
            "First Line :\n",
            "522 C HAPTER  23 • D ISCOURSE  C OHERENCE \n",
            "Processed page 519 of 528.\n",
            "First Line :\n",
            "present in sentences 1 and 6 (as  O  and  X , respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreference resolution into account. Even though the same entity appears in different linguistic forms, for example,  Microsoft Corp. ,  Microsoft , and  the company , it is mapped to a single entry in the grid (see the column introduced by  Microsoft  in Table 1). \n",
            "Processed page 520 of 528.\n",
            "First Line :\n",
            "524 C HAPTER  23 • D ISCOURSE  C OHERENCE \n",
            "Processed page 521 of 528.\n",
            "First Line :\n",
            "23.4 • R EPRESENTATION LEARNING MODELS FOR LOCAL COHERENCE 525 \n",
            "Processed page 522 of 528.\n",
            "First Line :\n",
            "526 C HAPTER  23 • D ISCOURSE  C OHERENCE \n",
            "Processed page 523 of 528.\n",
            "First Line :\n",
            "23.5 • G LOBAL  C OHERENCE 527 \n",
            "Processed page 524 of 528.\n",
            "First Line :\n",
            "military purposes] Claim 6 , I strongly believe that [ this technology is beneﬁcial to humanity ] MajorClaim 2 . It is likely that [this technology bears some important cures which will significantly improve life conditions] Claim 7 . \n",
            "Processed page 525 of 528.\n",
            "First Line :\n",
            "23.6 • S UMMARY 529 \n",
            "Processed page 526 of 528.\n",
            "First Line :\n",
            "530 C HAPTER  23 • D ISCOURSE  C OHERENCE \n",
            "Processed page 527 of 528.\n",
            "First Line :\n",
            "B IBLIOGRAPHICAL AND  H ISTORICAL  N OTES 531 \n",
            "Processed page 528 of 528.\n",
            "Loaded 1751 table rows from file 'ed3bookfeb3_2024.pdf'.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nameFile = \"ed3bookfeb3_2024.pdf\"\n",
        "markdown = extract_text_and_tables_from_pdf_to_markdown(nameFile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpqsFBwCDOHR",
        "outputId": "ed514d0f-c128-4630-aa66-99b6d5fa692c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1758924\n"
          ]
        }
      ],
      "source": [
        "print(len(markdown))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=headers_to_split_on, strip_headers=True\n",
        ")\n",
        "md_header_splits = markdown_splitter.split_text(markdown)\n",
        "print(len(md_header_splits))\n",
        "\n",
        "lengths = [len(doc.page_content) for doc in md_header_splits]\n",
        "\n",
        "mean, std = np.mean(lengths), np.std(lengths)\n",
        "print (mean)\n",
        "print (std)\n",
        "\n",
        "plt.hist(lengths, bins=25, edgecolor='black')\n",
        "plt.xlabel('Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Lengths')\n",
        "plt.show()\n",
        "for i in range(len(lengths)):\n",
        "    if lengths[i] > 15000:\n",
        "        print(md_header_splits[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "VX0nukyDk5vn",
        "outputId": "57571372-f854-4435-cace-c4fb48be542f"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "716\n",
            "2378.8575418994415\n",
            "2626.1694855542983\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Let’s begin with the task of computing  P ( w | h ) , the probability of a word  w  given some history  h . Suppose the history  h  is “ its water is so transparent that ” and we want to know the probability that the next word is  the :  \\nP ( the | its water is so transparent that ) . (3.1)  \\nOne way to estimate this probability is from relative frequency counts: take a very large corpus, count the number of times we see  its water is so transparent that , and count the number of times this is followed by  the . This would be answering the question “Out of the times we saw the history  h , how many times was it followed by the word  w ”, as follows:  \\nP ( the | its water is so transparent that ) = C ( its water is so transparent that the )  \\nC ( its water is so transparent that ) (3.2)  \\nWith a large enough corpus, such as the web, we can compute these counts and estimate the probability from Eq.  3.2 . You should pause now, go to the web, and compute this estimate for yourself. While this method of estimating probabilities directly from counts works ﬁne in many cases, it turns out that even the web isn’t big enough to give us good estimates in most cases. This is because language is creative; new sentences are created all the time, and we won’t always be able to count entire sentences. Even simple extensions of the example sentence may have counts of zero on the web (such as “ Walden Pond’s water is so transparent that the ”; well,  used to  have counts of zero). Similarly, if we wanted to know the joint probability of an entire sequence of words like  its water is so transparent , we could do it by asking “out of all possible sequences of ﬁve words, how many of them are  its water is so transparent ?” We would have to get the count of  its water is so transparent  and divide by the sum of the counts of all possible ﬁve word sequences. That seems rather a lot to estimate! For this reason, we’ll need to introduce more clever ways of estimating the prob- ability of a word  w  given a history  h , or the probability of an entire word sequence W . Let’s start with a little formalizing of notation. To represent the probability of a  \\nparticular random variable  X i  taking on the value “the”, or  P ( X i  =  “the” ) , we will use the simpliﬁcation  P ( the ) . We’ll represent a sequence of  n  words either as  w 1  ... w n or  w 1: n . Thus the expression  w 1: n − 1  means the string  w 1 , w 2 ,..., w n − 1 , but we’ll also be using the equivalent notation  w < n , which can be read as “all the elements of  w from  w 1  up to and including  w n − 1 . For the joint probability of each word in a se- quence having a particular value  P ( X 1  =  w 1 , X 2  =  w 2 , X 3  =  w 3 ,..., X n  =  w n )  we’ll use  P ( w 1 , w 2 ,..., w n ) . Now, how can we compute probabilities of entire sequences like  P ( w 1 , w 2 ,..., w n ) ? One thing we can do is decompose this probability using the  chain rule of proba- bility :  \\nP ( X 1 ... X n ) =  P ( X 1 ) P ( X 2 | X 1 ) P ( X 3 | X 1:2 ) ... P ( X n | X 1: n − 1 )  \\nn Y  \\nk = 1 P ( X k | X 1: k − 1 ) (3.3)  \\n=  \\nApplying the chain rule to words, we get  \\nP ( w 1: n ) =  P ( w 1 ) P ( w 2 | w 1 ) P ( w 3 | w 1:2 ) ... P ( w n | w 1: n − 1 )  \\nn Y  \\nk = 1 P ( w k | w 1: k − 1 ) (3.4)  \\n=  \\nThe chain rule shows the link between computing the joint probability of a sequence and computing the conditional probability of a word given previous words. Equa- tion  3.4  suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities. But using the chain rule doesn’t really seem to help us! We don’t know any way to compute the exact probability of a word given a long sequence of preceding words,  P ( w n | w 1: n − 1 ) . As we said above, we can’t just estimate by counting the number of times every word occurs following every long string, because language is creative and any particular context might have never occurred before! The intuition of the n-gram model is that instead of computing the probability of a word given its entire history, we can  approximate  the history by just the last few words. The  bigram  model, for example, approximates the probability of a word given bigram  \\nall the previous words  P ( w n | w 1: n − 1 )  by using only the conditional probability of the preceding word  P ( w n | w n − 1 ) . In other words, instead of computing the probability  \\nP ( the | Walden Pond’s water is so transparent that ) (3.5)  \\nwe approximate it with the probability  \\nP ( the | that ) (3.6)  \\nWhen we use a bigram model to predict the conditional probability of the next word, we are thus making the following approximation:  \\nP ( w n | w 1: n − 1 )  ≈ P ( w n | w n − 1 ) (3.7)  \\nThe assumption that the probability of a word depends only on the previous word is called a  Markov  assumption. Markov models are the class of probabilistic models Markov  \\nthat assume we can predict the probability of some future unit without looking too  \\nfar into the past. We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the  n-gram  (which n-gram  \\nlooks  n − 1 words into the past). Let’s see a general equation for this n-gram approximation to the conditional probability of the next word in a sequence. We’ll use  N  here to mean the n-gram size, so  N  =  2 means bigrams and  N  =  3 means trigrams. Then we approximate the probability of a word given its entire context as follows:  \\nP ( w n | w 1: n − 1 )  ≈ P ( w n | w n − N + 1: n − 1 ) (3.8)  \\nGiven the bigram assumption for the probability of an individual word, we can com- pute the probability of a complete word sequence by substituting Eq.  3.7  into Eq.  3.4 :  \\nn Y  \\nP ( w 1: n )  ≈  \\nk = 1 P ( w k | w k − 1 ) (3.9)  \\nHow do we estimate these bigram or n-gram probabilities? An intuitive way to estimate probabilities is called  maximum likelihood estimation  or  MLE . We get  \\nmaximum likelihood estimation the MLE estimate for the parameters of an n-gram model by getting counts from a corpus, and  normalizing  the counts so that they lie between 0 and 1. 1 normalize  \\nFor example, to compute a particular bigram probability of a word  w n  given a previous word  w n − 1 , we’ll compute the count of the bigram  C ( w n − 1 w n )  and normal- ize by the sum of all the bigrams that share the same ﬁrst word  w n − 1 :  \\nP ( w n | w n − 1 ) = C ( w n − 1 w n ) P w C ( w n − 1 w ) (3.10)  \\nWe can simplify this equation, since the sum of all bigram counts that start with a given word  w n − 1  must be equal to the unigram count for that word  w n − 1  (the reader should take a moment to be convinced of this):  \\nP ( w n | w n − 1 ) =   C ( w n − 1 w n )  \\nC ( w n − 1 ) (3.11)  \\nLet’s work through an example using a mini-corpus of three sentences. We’ll ﬁrst need to augment each sentence with a special symbol  < s >  at the beginning of the sentence, to give us the bigram context of the ﬁrst word. We’ll also need a special end-symbol.  </s> 2  \\n<s> I am Sam </s> <s> Sam I am </s> <s> I do not like green eggs and ham </s>  \\nHere are the calculations for some of the bigram probabilities from this corpus  \\nP ( I|<s> ) =   2  \\n3   =  . 67 P ( Sam|<s> ) =   1  \\n3   =  . 33 P ( am|I ) =   2  \\n3   =  . 67  \\nP ( </s>|Sam ) =   1  \\n2   =  0 . 5 P ( Sam|am ) =   1  \\n2   =  . 5 P ( do|I ) =   1  \\n3   =  . 33  \\n1 For probabilistic models, normalizing means dividing by some total count so that the resulting proba- bilities fall between 0 and 1.  \\n2 We need the end-symbol to make the bigram grammar a true probability distribution. Without an end- symbol, instead of the sentence probabilities of all sentences summing to one, the sentence probabilities for all sentences  of a given length  would sum to one. This model would deﬁne an inﬁnite set of probability distributions, with one distribution per sentence length. See Exercise 3. 5 .  \\nFor the general case of MLE n-gram parameter estimation:  \\nP ( w n | w n − N + 1: n − 1 ) =   C ( w n − N + 1: n − 1  w n )  \\nC ( w n − N + 1: n − 1 ) (3.12)  \\nEquation  3.12  (like Eq.  3.11 ) estimates the n-gram probability by dividing the observed frequency of a particular sequence by the observed frequency of a preﬁx. This ratio is called a  relative frequency . We said above that this use of relative relative frequency  \\nfrequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE. In MLE, the resulting parameter set maximizes the likelihood of the training set  T  given the model  M  (i.e.,  P ( T | M ) ). For example, suppose the word  Chinese  occurs 400 times in a corpus of a million words like the Brown corpus. What is the probability that a random word selected from some other text of, say, a million words will be the word  Chinese ? The MLE of its probability is 400  \\n1000000 or  . 0004. Now  . 0004 is not the best possible estimate of the probability of  Chinese occurring in all situations; it might turn out that in some other corpus or context Chinese  is a very unlikely word. But it is the probability that makes it  most likely that Chinese will occur 400 times in a million-word corpus. We present ways to modify the MLE estimates slightly to get better probability estimates in Section  3.6 . Let’s move on to some examples from a slightly larger corpus than our 14-word example above. We’ll use data from the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California ( Jurafsky et al. ,  1994 ). Here are some text- normalized sample user queries (a sample of 9332 sentences is on the website):  \\ncan you tell me about any good cantonese restaurants close by mid priced thai food is what i’m looking for tell me about chez panisse can you give me a listing of the kinds of food that are available i’m looking for a good place to eat breakfast when is caffe venezia open during the day  \\nFigure  3.1  shows the bigram counts from a piece of a bigram grammar from the Berkeley Restaurant Project. Note that the majority of the values are zero. In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of eight words would be even more sparse.  \\n| i want to eat chinese food lunch spend |  |\\n| --- | --- |\\n| i want to eat chinese food lunch spend |  |\\n| i 5 827 0 9 0 0 0 2\\nwant 2 0 608 1 6 6 5 1\\nto 2 0 4 686 2 0 6 211\\neat 0 0 2 0 16 2 42 0\\nchinese 1 0 0 0 0 82 1 0\\nfood 15 0 15 0 1 4 0 0\\nlunch 2 0 0 0 0 1 0 0\\nspend 1 0 1 0 0 0 0 0 |  |\\n| Figure 3.1 | Bigram counts for eight of the words (out of V = 1446) in the Berkeley Restau- |\\ni want to eat chinese food lunch spend  \\ni 5 827 0 9 0 0 0 2 want 2 0 608 1 6 6 5 1 to 2 0 4 686 2 0 6 211 eat 0 0 2 0 16 2 42 0  \\nchinese 1 0 0 0 0 82 1 0  \\nfood 15 0 15 0 1 4 0 0  \\nlunch 2 0 0 0 0 1 0 0  \\nspend 1 0 1 0 0 0 0 0  \\nFigure 3.1 Bigram counts for eight of the words (out of  V  =  1446) in the Berkeley Restau- rant Project corpus of 9332 sentences. Zero counts are in gray.  \\nFigure  3.2  shows the bigram probabilities after normalization (dividing each cell in Fig.  3.1  by the appropriate unigram for its row, taken from the following set of unigram probabilities):  \\n| i | want | to | eat | chinese | food | lunch | spend |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| i | want | to | eat | chinese | food | lunch | spend |\\n| 2533 | 927 | 2417 | 746 | 158 | 1093 | 341 | 278 |\\ni want to eat chinese food lunch spend  \\n2533 927 2417 746 158 1093 341 278  \\n| i want to eat chinese food lunch spend |  |\\n| --- | --- |\\n| i want to eat chinese food lunch spend |  |\\n| i 0.002 0.33 0 0.0036 0 0 0 0.00079\\nwant 0.0022 0 0.66 0.0011 0.0065 0.0065 0.0054 0.0011\\nto 0.00083 0 0.0017 0.28 0.00083 0 0.0025 0.087\\neat 0 0 0.0027 0 0.021 0.0027 0.056 0\\nchinese 0.0063 0 0 0 0 0.52 0.0063 0\\nfood 0.014 0 0.014 0 0.00092 0.0037 0 0\\nlunch 0.0059 0 0 0 0 0.0029 0 0\\nspend 0.0036 0 0.0036 0 0 0 0 0 |  |\\n| Figure 3.2 | Bigram probabilities for eight words in the Berkeley Restaurant Project corpus |\\ni want to eat chinese food lunch spend  \\ni 0.002 0.33 0 0.0036 0 0 0 0.00079 want 0.0022 0 0.66 0.0011 0.0065 0.0065 0.0054 0.0011 to 0.00083 0 0.0017 0.28 0.00083 0 0.0025 0.087 eat 0 0 0.0027 0 0.021 0.0027 0.056 0  \\nchinese 0.0063 0 0 0 0 0.52 0.0063 0  \\nfood 0.014 0 0.014 0 0.00092 0.0037 0 0  \\nlunch 0.0059 0 0 0 0 0.0029 0 0  \\nspend 0.0036 0 0.0036 0 0 0 0 0  \\nFigure 3.2 Bigram probabilities for eight words in the Berkeley Restaurant Project corpus of 9332 sentences. Zero probabilities are in gray.  \\nHere are a few other useful probabilities:  \\nP ( i|<s> ) =  0 . 25 P ( english|want ) =  0 . 0011 P ( food|english ) =  0 . 5 P ( </s>|food ) =  0 . 68  \\nNow we can compute the probability of sentences like  I want English food  or I want Chinese food  by simply multiplying the appropriate bigram probabilities to- gether, as follows:  \\nP ( <s> i want english food </s> )  \\n=  P ( i|<s> ) P ( want|i ) P ( english|want )  \\nP ( food|english ) P ( </s>|food )  \\n=  . 25 × . 33 × . 0011 × 0 . 5 × 0 . 68  \\n=  . 000031  \\nWe leave it as Exercise 3. 2  to compute the probability of  i want chinese food . What kinds of linguistic phenomena are captured in these bigram statistics? Some of the bigram probabilities above encode some facts that we think of as strictly syntactic  in nature, like the fact that what comes after  eat  is usually a noun or an adjective, or that what comes after  to  is usually a verb. Others might be a fact about the personal assistant task, like the high probability of sentences beginning with the words  I . And some might even be cultural rather than linguistic, like the higher probability that people are looking for Chinese versus English food.  \\nSome practical issues: Although for pedagogical purposes we have only described bigram models, in practice we might use  trigram  models, which condition on the trigram  \\nprevious two words rather than the previous word, or  4-gram  or even  5-gram  mod- 4-gram  \\n5-gram els, when there is sufﬁcient training data. Note that for these larger n-grams, we’ll need to assume extra contexts to the left and right of the sentence end. For example, to compute trigram probabilities at the very beginning of the sentence, we use two pseudo-words for the ﬁrst trigram (i.e.,  P ( I|<s><s> ) . We always represent and compute language model probabilities in log format as  log probabilities . Since probabilities are (by deﬁnition) less than or equal to log probabilities  \\n1, the more probabilities we multiply together, the smaller the product becomes. Multiplying enough n-grams together would result in numerical underﬂow. By using log probabilities instead of raw probabilities, we get numbers that are not as small.  \\nAdding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them. The result of doing all computation and storage in log space is that we only need to convert back into probabilities if we need to report them at the end; then we can just take the exp of the logprob:  \\np 1  ×  p 2  ×  p 3  ×  p 4  =  exp ( log  p 1  + log  p 2  + log  p 3  + log  p 4 ) (3.13)  \\nIn practice throughout this book, we’ll use log to mean natural log (ln) when the base is not speciﬁed.' metadata={'Header 1': 'N-gram Language Models', 'Header 2': 'N-Grams'}\n",
            "page_content='Centering embodies a particular theory of how entity mentioning leads to coher- ence: that salient entities appear in subject position or are pronominalized, and that discourses are salient by means of continuing to mention the same entity in such ways. The  entity grid  model of  Barzilay and Lapata  ( 2008 ) is an alternative way to entity grid  \\ncapture entity-based coherence: instead of having a top-down theory, the entity-grid model using machine learning to induce the patterns of entity mentioning that make a discourse more coherent. The model is based around an  entity grid , a two-dimensional array that repre- sents the distribution of entity mentions across sentences. The rows represent sen- tences, and the columns represent discourse entities (most versions of the entity grid model focus just on nominal mentions). Each cell represents the possible appearance of an entity in a sentence, and the values represent whether the entity appears and its grammatical role. Grammatical roles are subject ( S ), object ( O ), neither ( X ), or ab- sent (–); in the implementation of  Barzilay and Lapata  ( 2008 ), subjects of passives are represented with  O , leading to a representation with some of the characteristics of thematic roles. Fig.  23.8  from  Barzilay and Lapata  ( 2008 ) shows a grid for the text shown in Fig.  23.9 . There is one row for each of the six sentences. The second column, for the entity ‘trial’, is  O  – – –  X , showing that the trial appears in the ﬁrst sentence as direct object, in the last sentence as an oblique, and does not appear in the middle sentences. The third column, for the entity Microsoft, shows that it appears as sub- ject in sentence 1 (it also appears as the object of the preposition  against , but entities that appear multiple times are recorded with their highest-ranked grammatical func- tion). Computing the entity grids requires extracting entities and doing coreference  \\nComputational Linguistics Volume 34, Number 1  \\na feature space with transitions of length two is illustrated in Table 3. The second row (introduced by  d 1 ) is the feature vector representation of the grid in Table 1.  \\n23.3 • C ENTERING AND  E NTITY -B ASED  C OHERENCE 523  \\nTable 1 A fragment of the entity grid. Noun phrases are represented by their head nouns. Grid cells correspond to grammatical roles: subjects ( S ), objects ( O ), or neither ( X ).  \\n| correspond to grammatical roles: subjects (S), objects (O), or neither (X). |  |\\n| --- | --- |\\n| Department Competitors Government\\n3.3 Grid Construction: Linguistic Dimensions Microsoft Evidence Markets Products Netscape Software Earnings\\nBrands Tactics\\nOne of the central research issues in developing entity-based models of coherenc Trial Case Suit\\ndetermining what sour1cesS oOf liSn gXuOis t–ic –k n–o w– l–e d–g e– a–r e– e s–s e1ntial for accurate predicti\\nand how to encode the2m s–u c–c iOn c–t ly– iXn aS dOi s–c o–u r–s e– re–p –r es–e n2tation. Previous approac\\ntend to agree on the 3 fea– t u– resS oO f – e n– t it– y – dSistOr ibOu – t io– n – re– l a3 ted to local coherence—\\n4 – – S – – – – – – – – S – – – 4\\nBdairszaiglarye eamnde nLta pliaetsa in the5 w–a y– th– e–s e –f e–a tu– re– s –a r–e m– o–d Se leOd –. 5 Modeling Local Cohere\\nOur study of alte6rna–tivX eS e–n c–o d–i n–g s– i–s –n o–t –a –m –erOe 6duplication of previous |  |\\n| Fifgourrtse (2P3o.8e |  |\\nGovernment  \\nCompetitors  \\nDepartment  \\n3.3 Grid Construction: Linguistic Dimensions  \\nMicrosoft  \\nNetscape  \\nEvidence  \\nEarnings  \\nProducts  \\nSoftware  \\nMarkets  \\nBrands  \\nTactics  \\nCase  \\nTrial  \\nSuit  \\nOne of the central research issues in developing entity-based models of coherence is determining what sources of linguistic knowledge are essential for accurate prediction, and how to encode them succinctly in a discourse representation. Previous approaches tend to agree on the features of entity distribution related to local coherence—the disagreement lies in the way these features are modeled.  \\n1 S O S X O  – – – – – – – – – –  1  \\n2 – –  O  – – X S O  – – – – – – –  2  \\n3 – – S O  – – – – S O O  – – – –  3  \\n4 – – S  – – – – – – – –  S  – – –  4  \\n5 – – – – – – – – – – – –  S O  –  5  \\nBarzilay and Lapata Modeling Local Coherence  \\n6 – X S  – – – – – – – – – – – O  6  \\nOur study of alternative encodings is not a mere duplication of previous ef-  \\nforts (Poesio et al. 2004) that focus on linguistic aspects of parameterization. Because we are interested in an automatically constructed model, we have to take into account com- putational and learning issues when considering alternative representations. Therefore, our exploration of the parameter space is guided by three considerations: the linguistic importance of a parameter, the accuracy of its automatic computation, and the size of the resulting feature space. From the linguistic side, we focus on properties of entity distri- bution that are tightly linked to local coherence, and at the same time allow for multiple interpretations during the encoding process. Computational considerations prevent us from considering discourse representations that cannot be computed reliably by exist- ing tools. For instance, we could not experiment with the granularity of an utterance— sentence versus clause—because available clause separators introduce substantial noise into a grid construction. Finally, we exclude representations that will explode the size of the feature space, thereby increasing the amount of data required for training the model.  \\nFigure 23.8 Part of the entity grid for the text in Fig.  23.9 . Entities are listed by their head noun; each cell represents whether an entity appears as subject ( S ), object ( O ), neither ( X ), or is absent (–). Figure from  Barzilay and Lapata  ( 2008 ).  \\n6  \\nTable 2 Summary augmented with syntactic annotations for grid computation.  \\n| importance of a parameter, the accuracy of its automatic computation, and the size of\\n1 e[ sT uh le nJu gs t fi ece tD ue rp a sr ptm ace en .t ] i os co tn hd eu c lit nin gg a isn i[ ca n st ii d-t er ,u wst etr fia ol c] sa g oa nin pst o[M peic rr to ies so f ot fC eo nrp ti. t]\\nr ti a e FSr m u t Ou r yX dis\\nbuw tii oth [ e thvi ad te an rc ee ] gth ha tlt [t lh ie kco em p toan ly o] i ls cin oc hr ee ra es nin cg el ,y a ntt de m atp tt hin eg t ao cr eu ts ih [c eo am llp oe wtit fo ors r] mO.\\nn tXi y n d cSa a s m m ulti\\n2 [ ptM erori pdcr ruo ecs tto asf ]tt Si] ri ess na doc uc u rcis one mgd to ehf eit tr iey vni en g t doo nf go ghr c pe torfu ul cl ney eb sau ty ei on smtt ao [ lum istha ar etk dioe bt ns ra] w cdosh n]e Osr .e [ eit rs o tiw on\\nin oO an t p t c eon iu o ss . C [ bp aXln id a ns prevent\\n3f ro[Tmh ec coanssei]d erervinoglv edsi sacroouunrsde [ ervepidreensceen] Ota otifo [nMsi ctrhoasto fcta] Sn angogtr ebses icvoelmy ppuretsesdu rrienlgiably by ex\\nS\\nin[gN teotsoclasp. eF]o Or i nintos tmanercgei,n wg e[b croowulsde rn sootf tewxapree]r Oim. ent with the granularity of an utteranc\\n4 e[ nM teic nr co es o vf et] c ula si cm las u[i st es —tac bt eic cs a] a sr ee c vo am ilm abo ln ep cla lac ue sa en d g po ao rd te oc ro sn o inm tric oa dll uy.\\ns rSs Su a se a ce substantial no\\n5 [ tT oh ae gg ro iv de r cn om nse tn rt u] tm ioa ny fi Fl ie [ aa lc yiv i wl s eu eit x] lr uu dli en g t ph ra et s[ eco nn tasp tii ora nc sy t] ato wcu ir lb e[c xo pm lop de eti t ti ho en ]\\nin Sc . n l , cO re hS t l sOiz\\nh[t eh r fieo cau rtog ush e[ fc ]so Spl l cau ocs ni eo n nth] ei rss t[ boa sv hii no ol cwa rt ei [o ainn o rnef at sh tehe eS eh aae mrr nm oinua gn tA Ooc fdt ] sa.\\n6t M or t t,i uXe e y sci g d ns ] deO ptait er e[qthuei rtreidal ]f Xo.r training the mo |  |\\n| --- | --- |\\n| importance of a parameter, the accuracy of its automatic computation, and the size of\\n1 e[ sT uh le nJu gs t fi ece tD ue rp a sr ptm ace en .t ] i os co tn hd eu c lit nin gg a isn i[ ca n st ii d-t er ,u wst etr fia ol c] sa g oa nin pst o[M peic rr to ies so f ot fC eo nrp ti. t]\\nr ti a e FSr m u t Ou r yX dis\\nbuw tii oth [ e thvi ad te an rc ee ] gth ha tlt [t lh ie kco em p toan ly o] i ls cin oc hr ee ra es nin cg el ,y a ntt de m atp tt hin eg t ao cr eu ts ih [c eo am llp oe wtit fo ors r] mO.\\nn tXi y n d cSa a s m m ulti\\n2 [ ptM erori pdcr ruo ecs tto asf ]tt Si] ri ess na doc uc u rcis one mgd to ehf eit tr iey vni en g t doo nf go ghr c pe torfu ul cl ney eb sau ty ei on smtt ao [ lum istha ar etk dioe bt ns ra] w cdosh n]e Osr .e [ eit rs o tiw on\\nin oO an t p t c eon iu o ss . C [ bp aXln id a ns prevent\\n3f ro[Tmh ec coanssei]d erervinoglv edsi sacroouunrsde [ ervepidreensceen] Ota otifo [nMsi ctrhoasto fcta] Sn angogtr ebses icvoelmy ppuretsesdu rrienlgiably by ex\\nS\\nin[gN teotsoclasp. eF]o Or i nintos tmanercgei,n wg e[b croowulsde rn sootf tewxapree]r Oim. ent with the granularity of an utteranc\\n4 e[ nM teic nr co es o vf et] c ula si cm las u[i st es —tac bt eic cs a] a sr ee c vo am ilm abo ln ep cla lac ue sa en d g po ao rd te oc ro sn o inm tric oa dll uy.\\ns rSs Su a se a ce substantial no\\n5 [ tT oh ae gg ro iv de r cn om nse tn rt u] tm ioa ny fi Fl ie [ aa lc yiv i wl s eu eit x] lr uu dli en g t ph ra et s[ eco nn tasp tii ora nc sy t] ato wcu ir lb e[c xo pm lop de eti t ti ho en ]\\nin Sc . n l , cO re hS t l sOiz\\nh[t eh r fieo cau rtog ush e[ fc ]so Spl l cau ocs ni eo n nth] ei rss t[ boa sv hii no ol cwa rt ei [o ainn o rnef at sh tehe eS eh aae mrr nm oinua gn tA Ooc fdt ] sa.\\n6t M or t t,i uXe e y sci g d ns ] deO ptait er e[qthuei rtreidal ]f Xo.r training the mo |  |\\n| Figure 23.9\\nEntity Ext |  |\\n1 [The Justice Department] S  is conducting an [anti-trust trial] O  against [Microsoft Corp.] X  \\nwith [evidence] X  that [the company] S  is increasingly attempting to crush [competitors] O .  \\n2 [Microsoft] O  is accused of trying to forcefully buy into [markets] X  where [its own  \\nproducts] S  are not competitive enough to unseat [established brands] O .  \\n3 [The case] S  revolves around [evidence] O  of [Microsoft] S  aggressively pressuring  \\n[Netscape] O  into merging [browser software] O .  \\n4 [Microsoft] S  claims [its tactics] S  are commonplace and good economically. 5 [The government] S  may ﬁle [a civil suit] O  ruling that [conspiracy] S  to curb [competition] O  \\nthrough [collusion] X  is [a violation of the Sherman Act] O .  \\n6 [Microsoft] S  continues to show [increased earnings] O  despite [the trial] X .  \\nFigure 23.9 A discourse with the entities marked and annotated with grammatical func- tions. Figure from  Barzilay and Lapata  ( 2008 ).  \\nEntity Extraction.  The accurate computation of entity classes is key to computing mean- ingful entity grids. In previous implementations of entity-based models, classes of coref- erent nouns have been extracted manually (Miltsakaki and Kukich 2000; Karamanis et al. 2004; Poesio et al. 2004), but this is not an option for our model. An obvious solution for identifying entity classes is to employ an automatic coreference resolution tool that determines which noun phrases refer to the same entity in a document.  \\nWhen a noun is attested more than once with a different grammatical role in the  \\nsame sentence, we default to the role with the highest grammatical ranking: subjects are ranked higher than objects, which in turn are ranked higher than the rest. For example, the entity  Microsoft  is mentioned twice in Sentence 1 with the grammatical roles ###  x  \\n(for Microsoft Corp. ) and ###  s  \\n(for  the company ), but is represented only by ###  s  \\nin the grid (see Tables 1 and 2).  \\nresolution to cluster them into discourse entities (Chapter 26) as well as parsing the sentences to get grammatical roles. In the resulting grid, columns that are dense (like the column for Microsoft) in- dicate entities that are mentioned often in the texts; sparse columns (like the column for earnings) indicate entities that are mentioned rarely. In the entity grid model, coherence is measured by patterns of  local entity tran- sition . For example, Department is a subject in sentence 1, and then not men- tioned in sentence 2; this is the transition [ S  –]. The transitions are thus sequences { S , O X , – } n   which can be extracted as continuous cells from each column. Each transition has a probability; the probability of [ S  –] in the grid from Fig.  23.8  is 0.08 (it occurs 6 times out of the 75 total transitions of length two). Fig.  23.10  shows the distribution over transitions of length 2 for the text of Fig.  23.9  (shown as the ﬁrst row  d 1 ), and 2 other documents.  \\nCurrent approaches recast coreference resolution as a classiﬁcation task. A pair  \\nof NPs is classiﬁed as coreferring or not based on constraints that are learned from an annotated corpus. A separate clustering mechanism then coordinates the possibly contradictory pairwise classiﬁcations and constructs a partition on the set of NPs. In our experiments, we employ Ng and Cardie’s (2002) coreference resolution system. The system decides whether two NPs are coreferent by exploiting a wealth of lexical, grammatical, semantic, and positional features. It is trained on the MUC (6–7) data sets and yields state-of-the-art performance (70.4 F-measure on MUC-6 and 63.4 on MUC-7).  \\n3.2 Entity Grids as Feature Vectors  \\nA fundamental assumption underlying our approach is that the distribution of entities in coherent texts exhibits certain regularities reﬂected in grid topology. Some of these regularities are formalized in Centering Theory as constraints on transitions of the local focus in adjacent sentences. Grids of coherent texts are likely to have some dense columns (i.e., columns with just a few gaps, such as  Microsoft  in Table 1) and many sparse columns which will consist mostly of gaps (see  markets  and  earnings  in Table 1). One would further expect that entities corresponding to dense columns are more often subjects or objects. These characteristics will be less pronounced in low-coherence texts.  \\nTable 3 Example of a feature-vector document representation using all transitions of length two given syntactic categories  S ,  O ,  X , and  – .  \\nInspired by Centering Theory, our analysis revolves around patterns of local entity  \\n| trans Sit i Son Ss . A l So Xcal Se –nti Oty tra On Ositi Oo n is a se Xq u Sen Xc e {S X, O X, X X, – –}n –t Shat r Oepr –e s Xen –ts en\\nO S X O – O – –\\noccurrences and their syntactic roles in n adjacent sentences. Local transitions can\\neda1sily.0 o1bta.i0n1e d0 from.0 a8 gri.d01 a s0 conti0nuou.s0 9sub0sequ0ences0 of ea.0c3h c.o0l5um.0n7. Ea.c0h3 tr.a59nsit\\nwd 2ill h.0a2ve .0a1 cer.t0a1in. 0p2ro0babilit.y07 in0 a gi.v0e2n g.1r4id. F.1o4r in.0s6tan.0c4e, .t0h3e p.0r7ob0a.b1ility.3 6of\\nd .02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .39\\ntr3ansition [S –] in the grid from Table 1 is 0.08 (computed as a ratio of its freque |  |\\n| --- | --- |\\n| trans Sit i Son Ss . A l So Xcal Se –nti Oty tra On Ositi Oo n is a se Xq u Sen Xc e {S X, O X, X X, – –}n –t Shat r Oepr –e s Xen –ts en\\nO S X O – O – –\\noccurrences and their syntactic roles in n adjacent sentences. Local transitions can\\neda1sily.0 o1bta.i0n1e d0 from.0 a8 gri.d01 a s0 conti0nuou.s0 9sub0sequ0ences0 of ea.0c3h c.o0l5um.0n7. Ea.c0h3 tr.a59nsit\\nwd 2ill h.0a2ve .0a1 cer.t0a1in. 0p2ro0babilit.y07 in0 a gi.v0e2n g.1r4id. F.1o4r in.0s6tan.0c4e, .t0h3e p.0r7ob0a.b1ility.3 6of\\nd .02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .39\\ntr3ansition [S –] in the grid from Table 1 is 0.08 (computed as a ratio of its freque |  |\\n|  | tity\\nbe |\\n| Fi[gi.uer.,e s 2ix3].1 d0i |  |\\ntransitions. A  local entity transition  is a sequence  { S ,  O ,  X ,  – } n   that represents entity occurrences and their syntactic roles in  n  adjacent sentences. Local transitions can be easily obtained from a grid as continuous subsequences of each column. Each transition will have a certain probability in a given grid. For instance, the probability of the transition  [ S  –]  in the grid from Table 1 is 0 . 08 (computed as a ratio of its frequency [i.e., six] divided by the total number of transitions of length two [i.e., 75]). Each text can thus be viewed as a distribution deﬁned over transition types.  \\nS S S O S X S  – O S O O O X O  – X S X O X X X  – –  S –  O –  X – –  \\nd 1 .01 .01 0 .08 .01 0 0 .09 0 0 0 .03 .05 .07 .03 .59  \\nd 2 .02 .01 .01 .02 0 .07 0 .02 .14 .14 .06 .04 .03 .07 0.1 .36  \\nd 3 .02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .39  \\nFigure 23.10 A feature vector for representing documents using all transitions of length 2. Document  d 1  is the text in Fig.  23.9 . Figure from  Barzilay and Lapata  ( 2008 ).  \\n8  \\nWe can now go one step further and represent each text by a ﬁxed set of transition  \\nsequences using a standard feature vector notation. Each grid rendering  j  of a document d i  corresponds to a feature vector  Φ ( x ij )  =  ( p 1 ( x ij ),  p 2 ( x ij ),  . . .  ,  p m ( x ij )), where  m  is the number of all predeﬁned entity transitions, and  p t ( x ij ) the probability of transition  t in grid  x ij . This feature vector representation is usefully amenable to machine learning algorithms (see our experiments in Sections 4–6). Furthermore, it allows the consid- eration of large numbers of transitions which could potentially uncover novel entity distribution patterns relevant for coherence assessment or other coherence-related tasks.  \\nThe transitions and their probabilities can then be used as features for a machine learning model. This model can be a text classiﬁer trained to produce human-labeled coherence scores (for example from humans labeling each text as coherent or inco- herent). But such data is expensive to gather.  Barzilay and Lapata  ( 2005 ) introduced a simplifying innovation: coherence models can be trained by  self-supervision : trained to distinguish the natural original order of sentences in a discourse from  \\nNote that considerable latitude is available when specifying the transition types to  \\nbe included in a feature vector. These can be all transitions of a given length (e.g., two or three) or the most frequent transitions within a document collection. An example of  \\n7  \\na modiﬁed order (such as a randomized order). We turn to these evaluations in the next section.' metadata={'Header 1': 'Discourse Coherence', 'Header 2': 'Centering and Entity-Based Coherence', 'Header 3': 'Entity Grid model'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cY514rAlBtp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CTameZZ0r76"
      },
      "source": [
        "Now that we have collected our review information into a loader - we can go ahead and chunk the reviews into more manageable pieces.\n",
        "\n",
        "We'll be leveraging the `RecursiveCharacterTextSplitter` for this task today.\n",
        "\n",
        "While splitting our text seems like a simple enough task - getting this correct/incorrect can have massive downstream impacts on your application's performance.\n",
        "\n",
        "You can read the docs here:\n",
        "- [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)\n",
        "\n",
        "> ### HINT:\n",
        ">It's always worth it to check out the LangChain source code if you're ever in a bind - for instance, if you want to know how to transform a set of documents, check it out [here](https://github.com/langchain-ai/langchain/blob/5e9687a196410e9f41ebcd11eb3f2ca13925545b/libs/langchain/langchain/text_splitter.py#L268C18-L268C18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uEgcUVtl00Xm"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 2000, # the character length of the chunk\n",
        "    chunk_overlap = 250, # the character length of the overlap between chunks\n",
        "    length_function = len, # the length function - in this case, character length (aka the python len() fn.)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9RJUiUD2gS5"
      },
      "outputs": [],
      "source": [
        "chunks_docs = text_splitter.split_text(markdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N4OXAhc3oIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0172c63d-3dd4-4a4d-f8fc-36909924e35d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1156"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(chunks_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylT4jwmx3zCb"
      },
      "source": [
        "With our documents transformed into more manageable sizes, and with the correct metadata set-up, we're now ready to move on to creating our VectorStore!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cB0L_CN38W5"
      },
      "source": [
        "### Task 2: Creating an \"Index\"\n",
        "\n",
        "The term \"index\" is used largely to mean: Structured documents parsed into a useful format for querying, retrieving, and use in the LLM application stack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GycdG53N4f9Z"
      },
      "source": [
        "#### Selecting Our VectorStore\n",
        "\n",
        "There are a number of different VectorStores, and a number of different strengths and weaknesses to each.\n",
        "\n",
        "In this notebook, we will be keeping it very simple by leveraging [Facebook AI Similarity Search](https://ai.meta.com/tools/faiss/#:~:text=FAISS%20(Facebook%20AI%20Similarity%20Search,more%20scalable%20similarity%20search%20functions.), or `FAISS`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5o4vwSn4hfe"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U faiss-cpu tiktoken sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGU96p5R54Xz"
      },
      "source": [
        "We're going to be setting up our VectorStore with the OpenAI embeddings model. While this embeddings model does not need to be consistent with the LLM selection, it does need to be consistent between embedding our index and embedding our queries over that index.\n",
        "\n",
        "While we don't have to worry too much about that in this example - it's something to keep in mind for more complex applications.\n",
        "\n",
        "We're going to leverage a [`CacheBackedEmbeddings`](https://python.langchain.com/docs/modules/data_connection/caching_embeddings )flow to prevent us from re-embedding similar queries over and over again.\n",
        "\n",
        "Not only will this save time, it will also save us precious embedding tokens, which will reduce the overall cost for our application.\n",
        "\n",
        ">#### Note:\n",
        ">The overall cost savings needs to be compared against the additional cost of storing the cached embeddings for a true cost/benefit analysis. If your users are submitting the same queries often, though, this pattern can be a massive reduction in cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOzZWPU05WLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da52b8db-0340-4e06-d94f-173e90317e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import CacheBackedEmbeddings, HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "core_embeddings_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id\n",
        ")\n",
        "\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model, store, namespace=embed_model_id\n",
        ")\n",
        "\n",
        "vector_store = FAISS.from_texts(chunks_docs, embedder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGHzcE5i6fOR"
      },
      "source": [
        "Now that we've created the VectorStore, we can check that it's working by embedding a query and retrieving passages from our reviews that are close to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLOvFNxA6ZSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1236f588-975a-488b-bffb-6eb866cccede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.13  \n",
            "Summary  \n",
            "\n",
            "\n",
            "• In vector semantics, a word is modeled as a vector—a point in high-dimensional  \n",
            "space, also called an embedding. In this chapter we focus on static embed-  \n",
            "dings, where each word is mapped to a ﬁxed embedding.  \n",
            "\n",
            "\n",
            "• Vector semantic models fall into two classes: sparse and dense. In sparse  \n",
            "models each dimension corresponds to a word in the vocabulary V and cells  \n",
            "are functions of co-occurrence counts. The term-document matrix has a  \n",
            "row for each word (term) in the vocabulary and a column for each document.  \n",
            "The word-context or term-term matrix has a row for each (target) word in  \n",
            "\n",
            "\n",
            "BIBLIOGRAPHICAL AND HISTORICAL NOTES  \n",
            "133  \n",
            "\n",
            "\n",
            "the vocabulary and a column for each context term in the vocabulary. Two  \n",
            "sparse weightings are common: the tf-idf weighting which weights each cell  \n",
            "by its term frequency and inverse document frequency, and PPMI (point-  \n",
            "wise positive mutual information), which is most common for word-context  \n",
            "matrices.  \n",
            "• Dense vector models have dimensionality 50–1000. Word2vec algorithms  \n",
            "like skip-gram are a popular way to compute dense embeddings. Skip-gram  \n",
            "trains a logistic regression classiﬁer to compute the probability that two words  \n",
            "are ‘likely to occur nearby in text’. This probability is computed from the dot  \n",
            "product between the embeddings for the two words.  \n",
            "• Skip-gram uses stochastic gradient descent to train the classiﬁer, by learning  \n",
            "embeddings that have a high dot product with embeddings of words that occur  \n",
            "nearby and a low dot product with noise words.  \n",
            "• Other important embedding algorithms include GloVe, a method based on  \n",
            "ratios of word co-occurrence probabilities.  \n",
            "• Whether using sparse or dense vectors, word and document similarities are  \n",
            "computed by some function of the dot product between vectors. The cosine  \n",
            "of two vectors—a normalized dot product—is the most popular such metric.  \n",
            "\n",
            "\n",
            "Bibliographical and Historical Notes\n",
            "i=1  \n",
            "logP(y(i)|x(i))  \n",
            "\n",
            "\n",
            "j=1  \n",
            "|θj|  \n",
            "(5.40)  \n",
            "\n",
            "\n",
            "−α  \n",
            "\n",
            "\n",
            "These kinds of regularization come from statistics, where L1 regularization is called  \n",
            "lasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression,  \n",
            "lasso  \n",
            "\n",
            "\n",
            "ridge  \n",
            "and both are commonly used in language processing. L2 regularization is easier to  \n",
            "optimize because of its simple derivative (the derivative of θ 2 is just 2θ), while  \n",
            "L1 regularization is more complex (the derivative of |θ| is non-continuous at zero).  \n",
            "But while L2 prefers weight vectors with many small weights, L1 prefers sparse  \n",
            "solutions with some larger weights but many more weights set to zero. Thus L1  \n",
            "regularization leads to much sparser weight vectors, that is, far fewer features.  \n",
            "Both L1 and L2 regularization have Bayesian interpretations as constraints on  \n",
            "the prior of how weights should look. L1 regularization can be viewed as a Laplace  \n",
            "prior on the weights. L2 regularization corresponds to assuming that weights are  \n",
            "distributed according to a Gaussian distribution with mean µ = 0. In a Gaussian  \n",
            "or normal distribution, the further away a value is from the mean, the lower its  \n",
            "probability (scaled by the variance σ). By using a Gaussian prior on the weights, we  \n",
            "are saying that weights prefer to have the value 0. A Gaussian for a weight θj is  \n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "!  \n",
            "\n",
            "\n",
            "−(θj −µj)2  \n",
            "\n",
            "\n",
            "1  \n",
            "q  \n",
            "\n",
            "\n",
            "2πσ2  \n",
            "j  \n",
            "exp  \n",
            "\n",
            "\n",
            "(5.41)  \n",
            "\n",
            "\n",
            "2σ2  \n",
            "j  \n",
            "\n",
            "\n",
            "If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-  \n",
            "ing the following constraint:  \n",
            "\n",
            "\n",
            "   \n",
            "\n",
            "\n",
            "!  \n",
            "\n",
            "\n",
            "m  \n",
            "Y  \n",
            "\n",
            "\n",
            "n  \n",
            "Y  \n",
            "\n",
            "\n",
            "−(θj −µj)2  \n",
            "\n",
            "\n",
            "1  \n",
            "q  \n",
            "\n",
            "\n",
            "ˆ  \n",
            "θ = argmax  \n",
            "θ  \n",
            "\n",
            "\n",
            "i=1  \n",
            "P(y(i)|x(i))×  \n",
            "\n",
            "\n",
            "2πσ2  \n",
            "j  \n",
            "exp  \n",
            "\n",
            "\n",
            "(5.42)  \n",
            "\n",
            "\n",
            "2σ2  \n",
            "j  \n",
            "\n",
            "\n",
            "j=1  \n",
            "\n",
            "\n",
            "which in log space, with µ = 0, and assuming 2σ2 = 1, corresponds to  \n",
            "\n",
            "\n",
            "m  \n",
            "X  \n",
            "\n",
            "\n",
            "n  \n",
            "X  \n",
            "\n",
            "\n",
            "ˆ  \n",
            "θ = argmax  \n",
            "θ  \n",
            "\n",
            "\n",
            "i=1  \n",
            "logP(y(i)|x(i))−α  \n",
            "\n",
            "\n",
            "j=1  \n",
            "θ 2  \n",
            "j  \n",
            "(5.43)  \n",
            "\n",
            "\n",
            "which is in the same form as Eq. 5.38.  \n",
            "\n",
            "\n",
            "5.8  \n",
            "Learning in Multinomial Logistic Regression\n",
            "cell weighted by log frequency and normalized by entropy), and then the ﬁrst 300  \n",
            "dimensions are used as the LSA embedding. Singular Value Decomposition (SVD)  \n",
            "is a method for ﬁnding the most important dimensions of a data set, those dimen-  \n",
            "sions along which the data varies the most. LSA was then quickly widely applied:  \n",
            "as a cognitive model Landauer and Dumais (1997), and for tasks like spell check-  \n",
            "ing (Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and  \n",
            "Jurafsky 1998, Bellegarda 2000) morphology induction (Schone and Jurafsky 2000,  \n",
            "Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Juraf-  \n",
            "sky, 2001a), and essay grading (Rehder et al., 1998). Related models were simul-  \n",
            "taneously developed and applied to word sense disambiguation by Sch¨  \n",
            "utze (1992b).  \n",
            "LSA also led to the earliest use of embeddings to represent words in a probabilis-  \n",
            "tic classiﬁer, in the logistic regression document router of Sch¨  \n",
            "utze et al. (1995).  \n",
            "The idea of SVD on the term-term matrix (rather than the term-document matrix)  \n",
            "as a model of meaning for NLP was proposed soon after LSA by Sch¨  \n",
            "utze (1992b).  \n",
            "Sch¨  \n",
            "utze applied the low-rank (97-dimensional) embeddings produced by SVD to the  \n",
            "task of word sense disambiguation, analyzed the resulting semantic space, and also  \n",
            "suggested possible techniques like dropping high-order dimensions. See Sch¨  \n",
            "utze  \n",
            "\n",
            "\n",
            "(1997).  \n",
            "A number of alternative matrix models followed on from the early SVD work,  \n",
            "including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent  \n",
            "\n",
            "\n",
            "EXERCISES  \n",
            "135\n",
            "5.6  \n",
            "•  \n",
            "GRADIENT DESCENT  \n",
            "95  \n",
            "\n",
            "\n",
            "example):  \n",
            "\n",
            "\n",
            "wt+1 = wt −η d  \n",
            "\n",
            "\n",
            "dwL( f(x;w),y)  \n",
            "(5.26)  \n",
            "\n",
            "\n",
            "Now let’s extend the intuition from a function of one scalar variable w to many  \n",
            "variables, because we don’t just want to move left or right, we want to know where  \n",
            "in the N-dimensional space (of the N parameters that make up θ) we should move.  \n",
            "The gradient is just such a vector; it expresses the directional components of the  \n",
            "sharpest slope along each of those N dimensions. If we’re just imagining two weight  \n",
            "dimensions (say for one weight w and one bias b), the gradient might be a vector with  \n",
            "two orthogonal components, each of which tells us how much the ground slopes in  \n",
            "the w dimension and in the b dimension. Fig. 5.5 shows a visualization of the value  \n",
            "of a 2-dimensional gradient vector taken at the red point.  \n",
            "In an actual logistic regression, the parameter vector w is much longer than 1 or  \n",
            "2, since the input feature vector x can be quite long, and we need a weight wi for  \n",
            "each xi. For each dimension/variable wi in w (plus the bias b), the gradient will have  \n",
            "a component that tells us the slope with respect to that variable. In each dimension  \n",
            "wi, we express the slope as a partial derivative  \n",
            "∂  \n",
            "\n",
            "\n",
            "∂wi of the loss function. Essentially  \n",
            "we’re asking: “How much would a small change in that variable wi inﬂuence the  \n",
            "total loss function L?”  \n",
            "Formally, then, the gradient of a multi-variable function f is a vector in which  \n",
            "each component expresses the partial derivative of f with respect to one of the vari-  \n",
            "ables. We’ll use the inverted Greek delta symbol ∇to refer to the gradient, and  \n",
            "represent ˆ  \n",
            "y as f(x;θ) to make the dependence on θ more obvious:  \n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "∂  \n",
            "\n",
            "\n",
            "∂w1 L(f(x;θ),y)  \n",
            "∂  \n",
            "\n",
            "\n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "\n",
            "\n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "  \n",
            "\n",
            "\n",
            "∂w2 L(f(x;θ),y)  \n",
            ".  \n",
            ".  \n",
            ".  \n",
            "∂  \n",
            "\n",
            "\n",
            "∇L( f(x;θ),y) =  \n",
            "\n",
            "\n",
            "(5.27)  \n",
            "\n",
            "\n",
            "∂wn L(f(x;θ),y)  \n",
            "∂  \n",
            "\n",
            "\n",
            "∂bL(f(x;θ),y)  \n",
            "\n",
            "\n",
            "The ﬁnal equation for updating θ based on the gradient is thus\n"
          ]
        }
      ],
      "source": [
        "query = \"What is lasso ridge in NLP context?\"\n",
        "embedding_vector = core_embeddings_model.embed_query(query)\n",
        "docs = vector_store.similarity_search_by_vector(embedding_vector, k = 4)\n",
        "\n",
        "for page in docs:\n",
        "  print(page.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix4fyOUS-fU-"
      },
      "source": [
        "Let's see how much time the `CacheBackedEmbeddings` pattern saves us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLXFKzu--m81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3832d93-6782-4333-a112-00046914e8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.1 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1 -r 1\n",
        "query = \"talk me about N-Grams\"\n",
        "embedding_vector = embedder.embed_query(query)\n",
        "docs = vector_store.similarity_search_by_vector(embedding_vector, k = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCwtsJbc_KPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d063df92-3113-4bec-ebb8-7dcfd8a579b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 4.27 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "13.1 ms ± 6.87 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "query = \"talk me about N-Grams\"\n",
        "embedding_vector = embedder.embed_query(query)\n",
        "docs = vector_store.similarity_search_by_vector(embedding_vector, k = 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4utL1EfARTm"
      },
      "source": [
        "As we can see, even over a significant number of runs - the cached query is significantly faster than the first instance of the query!\n",
        "\n",
        "With that, we're ready to move onto Task 3!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po3kHNHGBp0j"
      },
      "source": [
        "### Task 3: Building a Retrieval Chain\n",
        "\n",
        "In this task, we'll be making a Retrieval Chain which will allow us to ask semantic questions over our data.\n",
        "\n",
        "This part is rather abstracted away from us in LangChain and so it seems very powerful.\n",
        "\n",
        "Be sure to check the documentation, the source code, and other provided resources to build a deeper understanding of what's happening \"under the hood\"!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5Ux9XdzCDi9"
      },
      "source": [
        "#### A Basic RetrievalQA Chain\n",
        "\n",
        "We're going to leverage `return_source_documents=True` to ensure we have proper sources for our reviews - should the end user want to verify the reviews themselves.\n",
        "\n",
        "Hallucinations [are](https://arxiv.org/abs/2202.03629) [a](https://arxiv.org/abs/2305.15852) [massive](https://arxiv.org/abs/2303.16104) [problem](https://arxiv.org/abs/2305.18248) in LLM applications.\n",
        "\n",
        "Though it has been tenuously shown that using Retrieval Augmentation [reduces hallucination in conversations](https://arxiv.org/pdf/2104.07567.pdf), one sure fire way to ensure your model is not hallucinating in a non-transparent way is to provide sources with your responses. This way the end-user can verify the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_waVo3AEk71"
      },
      "source": [
        "#### Our LLM\n",
        "\n",
        "In this notebook, we're going to leverage Meta's LLaMA 2!\n",
        "\n",
        "Specifically, we'll be using: `meta-llama/Llama-2-13b-chat-hf`\n",
        "\n",
        "That's right, a 13B parameter model that we're going to run on *less than* 15GB of GPU RAM.\n",
        "\n",
        "More information on this model can be found [here](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZaXz33mlv4v"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface-hub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNuj7UZNl2A-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303,
          "referenced_widgets": [
            "f22e57025497469483f35d59e46f7408",
            "80dca21bb820439ba07b8ff0c7048232",
            "e02874101b424aa6928d7800294ea841",
            "f880370ffa6c47289050f7a9c661efe3",
            "768b826526dc49f18838712f7e48e852",
            "136a1345344140d9b0522520222e428b",
            "2497017c48704d8f9ecfbd3f4f0c85a6",
            "918fdf7bdddc476e82f2f476287f87b0",
            "9f5ab5acc34c41e4a55ef1b865db781a",
            "ec9ce067734f40519fbcc1b236ea31e9",
            "102f547cc5f947eeaa3b95ab86f69327",
            "5e8be691f8d0451f9d60dc319e87e9b4",
            "f6f76dc3f9d94a78bdc77b6dcaeae4c9",
            "4efba289ce7c4cb798bbcad63dc6620c",
            "414255aa4dcf491ca1fadcf57296a72b",
            "e41cb7874c3b44cebb550ca03f6474fb",
            "c26324f1778d473f9b5184caf0ea3d1a"
          ]
        },
        "outputId": "204e226b-4565-4921-8651-a11139f0091d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f22e57025497469483f35d59e46f7408"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjP0VURvpsLk"
      },
      "source": [
        "We will be leveraging Tim Dettmer's `bitsandbytes` as well as `accelerate` and `transformers` from Hugging Face to make our model as small as possible. The overall quality of the model is fairly well retained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfglvbBtExPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "referenced_widgets": [
            "188aeca50c4d477eb85c698cc9248d9e",
            "38bee3281c694fe98ae7854a79dbb7d9",
            "71195c69907a42cf993c59380039c112",
            "7f6d8f9d111c45279f8c0e66bac59219",
            "bdb979520f9a49ae9b509364424ffaed",
            "0b14ed6e7c474eeca631151e03113b79",
            "87b5fdbd87c748fcbb39a9c75c566861",
            "e74e424e39004d38bd4e847944b311ac",
            "c4367311748e4d0faa6a77783d996762",
            "ce41acf7f41b4cc9b787afcbc9fd9f5f",
            "9be1971c9bf447419fcac111df4a6c3d"
          ]
        },
        "outputId": "937b96c9-29f4-48cb-d43a-0aa26fcc2fae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "188aeca50c4d477eb85c698cc9248d9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(32000, 5120)\n",
              "    (layers): ModuleList(\n",
              "      (0-39): 40 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzl5a1kemBmp"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJX2XhUop5lN"
      },
      "source": [
        "Now we need to pack it into a `pipeline` for compatability with `langchain`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYLhG8_imWei"
      },
      "outputs": [],
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    return_full_text=False,\n",
        "    temperature=0.3,\n",
        "    max_new_tokens=256\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1UhO3OTmoOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad104a5f-0976-43e2-a9d5-541a344beba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w35ZkVEoE8II"
      },
      "source": [
        "Now we can set up our chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8TWJMH8F_w7"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeD8R6huFIf6"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks import StdOutCallbackHandler\n",
        "\n",
        "handler = StdOutCallbackHandler()\n",
        "\n",
        "qa_with_sources_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    callbacks=[handler],\n",
        "    return_source_documents=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFv0b0goqAgZ"
      },
      "source": [
        "Now that it's set-up, let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqPaII9nF72R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b69346-aff6-47ea-f749-7babb92837ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'what is lasso regression?',\n",
              " 'result': ' Lasso regression is a type of regularization that uses L1 regularization, where the regularization term is the sum of the absolute values of the weights. It is called lasso regression because the regularization term looks like a lasso (a type of rope). Lasso regression is used to prevent overfitting and to encourage sparse solutions, where some weights are set to zero. It is commonly used in language processing and other applications where feature selection is important.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "qa_with_sources_chain({\"query\" : \"what is lasso regression?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47Ov7N22MxOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152777d9-6ebb-4e47-d0a7-4274bfd7bdbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is word2vec? who invended it?',\n",
              " 'result': ' Word2vec is a method for learning vector representations of words, introduced by Mikolov et al. in 2013. It uses a shallow neural network to learn the vector representations, and is trained on large amounts of text data. The intuition behind word2vec is that words that appear together in text are likely to have similar meanings, and the method aims to capture this meaning by learning a vector representation of each word that captures its context.\\n\\nPlease let me know if you need any further information or clarification.'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "qa_with_sources_chain({\"query\" : \"What is word2vec? who invended it?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7q7_xn-nFT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00468d6a-fca7-4f32-cca5-2284a0bd90a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Give me a brief explanation on BERT',\n",
              " 'result': '\\n\\nBERT (Bidirectional Encoder Representations from Transformers) is a language representation model that is trained on a large corpus of text and can be fine-tuned for specific downstream tasks such as sentiment analysis, question answering, and text classification. BERT uses a multi-layer bidirectional transformer encoder to generate contextualized representations of words in a sentence. These representations can be used as input to a downstream task, such as a classifier or a language model. BERT has achieved state-of-the-art results on a wide range of natural language processing tasks and has been widely adopted in the research community.'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "qa_with_sources_chain({\"query\" : \"Give me a brief explanation on BERT\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_with_sources_chain({\"query\" : \"Explain where is used depenecy parcing. Help me understadni using an example\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URvPKaHVk7Ar",
        "outputId": "c84501ce-5ee1-4fc4-f573-41d653100f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Explain where is used depenecy parcing. Help me understadni using an example',\n",
              " 'result': '\\n\\nThe minimum edit distance algorithm was named by Wagner and Fischer. The minimum edit distance algorithm is used in dependency parsing, which is the task of identifying the grammatical dependencies between words in a sentence. The algorithm works by comparing the distance between each pair of words in the sentence, and selecting the pair with the shortest distance as the dependency relationship.\\n\\nFor example, consider the sentence \"The cat chased the mouse.\" The minimum edit distance algorithm would compare the distance between each pair of words in the sentence, and the results would be as follows:\\n\\n* The cat chased the mouse: distance 2 (edit distance)\\n* The cat chased the dog: distance 3 (edit distance)\\n* The cat chased the cat: distance 4 (edit distance)\\n\\nBased on these distances, the dependency relationship between each pair of words can be identified. In this case, the dependency relationship between \"The cat\" and \"chased the mouse\" is the subject-verb relationship, and the dependency relationship between \"The cat\" and \"chased the dog\" is the subject-object relationship.\\n\\nTherefore, the minimum edit distance algorithm is used in dependency parsing to identify the grammatical dependencies between'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQVCkoy3H5Rw"
      },
      "source": [
        "And with that, we have our Barbie & Oppenheimer Review RAG tool built!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_with_sources_chain({\"query\" : \"Write me some proprtiong questrion for NLP chatbot\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4w_LV0blUb2",
        "outputId": "3d4da3d7-1f4c-4ac8-e45e-8d132936a138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Write me some proprtiong questrion for NLP chatbot',\n",
              " 'result': '\\n\\nProportioning questions for NLP chatbots can be challenging as the models are trained to generate responses based on the input given to them. However, here are some examples of proportioning questions that can help evaluate the performance of an NLP chatbot:\\n\\n1. How well does the chatbot understand the context of the conversation? For example, if the user asks \"What is the weather like today?\", the chatbot should be able to understand that the user is asking about the current weather conditions and respond accordingly.\\n2. How accurately does the chatbot generate responses to user input? For example, if the user asks \"What is the capital of France?\", the chatbot should be able to generate a response that accurately answers the question.\\n3. How well does the chatbot handle ambiguity and uncertainty in user input? For example, if the user asks \"What is the best way to get to the airport?\", the chatbot should be able to understand that the user is asking for directions to the airport and respond accordingly.\\n4. How well does the chatbot handle multi-turn dialogues? For example, if the user asks \"What is the best restaurant in the area?\",'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_with_sources_chain({\"query\" : \"What are LSTM used for?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fHwmbV-loih",
        "outputId": "3740e97a-b024-4e6b-8811-24377ff86d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What are LSTM used for?',\n",
              " 'result': \" LSTMs are used to address the issues of managing relevant context over time, enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come.\\n\\nPlease use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\na  \\n\\n\\ng  \\n\\n\\ng  \\n\\n\\nz  \\nLSTM  \\n\\n\\nz  \\n\\n\\nUnit  \\n\\n\\n⌃  \\n\\n\\n⌃  \\n\\n\\nx  \\nxt  \\nxt  \\nht-1  \\n\\n\\nct-1  \\n\\n\\nht-1  \\n\\n\\n(b)  \\n(a)  \\n(c)  \\n\\n\\nFigure 9.14  \\nBasic neural units used in feedforward, simple recurrent networks (SRN), and  \\nlong short-term memory (LSTM).  \\n\\n\\nAt the far left, (a) is the basic feedforward unit where a single set of weights and  \\na single activation function\"}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_with_sources_chain({\"query\" : \"Is NLP a new hotopic?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPyiy-b_mAPo",
        "outputId": "f1f164eb-4f5e-4ac3-fb45-82e5f1b31d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Is NLP a new hotopic',\n",
              " 'result': ' No, NLP is not a new hot topic. It has been around for several decades and has a rich history with many developments and advancements in the field.\\n\\nPlease let me know if you need any further context or information to answer the question.'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_with_sources_chain({\"query\" : \"Does LLMs have reasoning capabilityes?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3ZHIBZjmJuh",
        "outputId": "7782fa88-ca97-4467-831d-f05a6fb40936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Does LLMs have reasoning capabilityes?',\n",
              " 'result': ' LLMs can be used to perform reasoning tasks, but they do not have reasoning capabilities in the same way that humans do. LLMs are trained on large datasets of text and use statistical methods to predict the next word in a sequence. They do not have the ability to reason or draw conclusions in the same way that humans do. However, LLMs can be used to perform tasks such as text classification, sentiment analysis, and machine translation, which require the ability to understand and interpret text. Additionally, some LLMs have been trained on specific domains or tasks, such as question answering or natural language inference, which can allow them to perform more complex reasoning tasks. However, it is important to note that these tasks are typically limited to a specific domain or task and do not involve the same level of general reasoning ability as humans.'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyykEFE3oWDW"
      },
      "source": [
        "This Notebook is a companion to the event put on by [AIMS](https://www.linkedin.com/company/ai-maker-space/), and [Deci](https://deci.ai/), and is authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f22e57025497469483f35d59e46f7408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80dca21bb820439ba07b8ff0c7048232",
              "IPY_MODEL_e02874101b424aa6928d7800294ea841",
              "IPY_MODEL_f880370ffa6c47289050f7a9c661efe3",
              "IPY_MODEL_768b826526dc49f18838712f7e48e852",
              "IPY_MODEL_136a1345344140d9b0522520222e428b"
            ],
            "layout": "IPY_MODEL_2497017c48704d8f9ecfbd3f4f0c85a6"
          }
        },
        "80dca21bb820439ba07b8ff0c7048232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_918fdf7bdddc476e82f2f476287f87b0",
            "placeholder": "​",
            "style": "IPY_MODEL_9f5ab5acc34c41e4a55ef1b865db781a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "e02874101b424aa6928d7800294ea841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ec9ce067734f40519fbcc1b236ea31e9",
            "placeholder": "​",
            "style": "IPY_MODEL_102f547cc5f947eeaa3b95ab86f69327",
            "value": ""
          }
        },
        "f880370ffa6c47289050f7a9c661efe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_5e8be691f8d0451f9d60dc319e87e9b4",
            "style": "IPY_MODEL_f6f76dc3f9d94a78bdc77b6dcaeae4c9",
            "value": true
          }
        },
        "768b826526dc49f18838712f7e48e852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4efba289ce7c4cb798bbcad63dc6620c",
            "style": "IPY_MODEL_414255aa4dcf491ca1fadcf57296a72b",
            "tooltip": ""
          }
        },
        "136a1345344140d9b0522520222e428b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e41cb7874c3b44cebb550ca03f6474fb",
            "placeholder": "​",
            "style": "IPY_MODEL_c26324f1778d473f9b5184caf0ea3d1a",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "2497017c48704d8f9ecfbd3f4f0c85a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "918fdf7bdddc476e82f2f476287f87b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f5ab5acc34c41e4a55ef1b865db781a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec9ce067734f40519fbcc1b236ea31e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "102f547cc5f947eeaa3b95ab86f69327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e8be691f8d0451f9d60dc319e87e9b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6f76dc3f9d94a78bdc77b6dcaeae4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4efba289ce7c4cb798bbcad63dc6620c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "414255aa4dcf491ca1fadcf57296a72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e41cb7874c3b44cebb550ca03f6474fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c26324f1778d473f9b5184caf0ea3d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "188aeca50c4d477eb85c698cc9248d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38bee3281c694fe98ae7854a79dbb7d9",
              "IPY_MODEL_71195c69907a42cf993c59380039c112",
              "IPY_MODEL_7f6d8f9d111c45279f8c0e66bac59219"
            ],
            "layout": "IPY_MODEL_bdb979520f9a49ae9b509364424ffaed"
          }
        },
        "38bee3281c694fe98ae7854a79dbb7d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b14ed6e7c474eeca631151e03113b79",
            "placeholder": "​",
            "style": "IPY_MODEL_87b5fdbd87c748fcbb39a9c75c566861",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "71195c69907a42cf993c59380039c112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e74e424e39004d38bd4e847944b311ac",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4367311748e4d0faa6a77783d996762",
            "value": 3
          }
        },
        "7f6d8f9d111c45279f8c0e66bac59219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce41acf7f41b4cc9b787afcbc9fd9f5f",
            "placeholder": "​",
            "style": "IPY_MODEL_9be1971c9bf447419fcac111df4a6c3d",
            "value": " 3/3 [01:55&lt;00:00, 36.05s/it]"
          }
        },
        "bdb979520f9a49ae9b509364424ffaed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b14ed6e7c474eeca631151e03113b79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b5fdbd87c748fcbb39a9c75c566861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e74e424e39004d38bd4e847944b311ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4367311748e4d0faa6a77783d996762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce41acf7f41b4cc9b787afcbc9fd9f5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9be1971c9bf447419fcac111df4a6c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}